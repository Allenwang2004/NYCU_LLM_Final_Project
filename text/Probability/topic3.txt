
[Page 1]
Topic 3: Discrete Random Variables Topic Outline ï¬ Random Variables (RV, éš¨æ©Ÿè®Šæ•¸) Discrete Random Variables é›¢æ•£éš¨æ©Ÿè®Šæ•¸(Topic 3ï¼Œtextbook Chapter 2) Continuous Random Variables é€£çºŒéš¨æ©Ÿè®Šæ•¸(Topic 6, textbook Chapter 3) ï¬ Probability Mass Functions (PMF) of Discrete RVs ï¬ Typical Discrete Random Variables:  Uniform  Bernoulli and Binomial  Geometric  Poisson ï¬ Functions of Random Variables Reading: Textbook 2.1 â€“ 2.3
[Page 2]
Probability  Definition Mathematically: A random variable is an assignment (i.e., a mapping or, a function) of a real number to each sample point (i.e., outcome) in the sample space, or a function that maps sample space into the real line ç°¡è¨€ä¹‹ï¼Œéš¨æ©Ÿè®Šæ•¸æ˜¯ä¸€å€‹å‡½æ•¸ã€‚ Example: Flip a coin. We can define a random variable X, in a way that X(H)=1 and X(T)=0 What is Random Variable (RV)?
[Page 3]
Probability  éš¨æ©Ÿè®Šæ•¸æ˜¯ä¸€å€‹å‡½æ•¸ï¼Œå®ƒå°‡æ¨£æœ¬ç©ºé–“ä¸­çš„æ¯ä¸€å€‹outcome å°æ‡‰åˆ°æŸä¸€å€‹ç‰¹å®šçš„å¯¦æ•¸ Concept: Itâ€™s not merely a variable; it has a sense of randomness in it â€œRandomnessâ€ lies in the sample space in the sense that, before conducting an experiment, we are uncertain which outcome the experiment will produce. More precisely, itâ€™s a function Performing an experiment yields a specific sample point (outcome) Ï‰, which produces a sample value, say x = X(Ï‰), by means of the random variable. (Be aware of the difference between the capital letter X and the small letter x) ä¸€èˆ¬æˆ‘å€‘ç”¨æ–œé«”å¤§å¯«(X)è¡¨ç¤ºéš¨æ©Ÿè®Šæ•¸ï¼Œæ–œé«”å°å¯«(x)è¡¨ç¤ºé€™å€‹éš¨æ©Ÿè®Šæ•¸çš„ä¸€å€‹å¯èƒ½å¯¦æ•¸çµæœ What is Random Variable?
[Page 4]
Probability  1 2 3 4 Real Line ã€‚ ã€‚ ã€‚ ã€‚ 4 3 1 2 2nd roll 1st roll Example: Experiment: roll two 4-sided dice The maximum of the two throws can be considered as a random variable, X(a,b)=max(a,b), where a and b are the outcome of the first and second dice.
[Page 5]
Probability  ï¬For mathematical convenience We can describe complicated events using simple math expressions by means of random variables (åŒ–ç¹ç‚ºç°¡) Particularly useful when outcomes of the considered experiment do not involve with any numerical values, e.g. coin flips (Heads, Tails) Example: Flip a coin 3 times. Define the random variable Xi=1 if the ith flip comes Heads, and Xi=0 if Tails. â†’F={Two heads in 3 flips} ïƒ°F= ğ‘‹ğ‘‹1, ğ‘‹ğ‘‹2, ğ‘‹ğ‘‹3 : ğ‘‹ğ‘‹1 + ğ‘‹ğ‘‹2 + ğ‘‹ğ‘‹3 = 2, ğ‘‹ğ‘‹ğ‘–ğ‘–= 0 or 1 ; æ›´ç°¡æ½”ä¸€é»çš„å¯«æ³•æ˜¯F={ X1+X2+X3=2 } â†’G={1st flip is a head, 2nd and 3rd flips have different results} ïƒ°G={ X1=1, X2â‰ X3 } â†’Every event has its particular physical meanings, and can be described precisely and elegantly by properly designed random variables (functions) Why the Notion of â€œRandom Variableâ€? HHH THH HHT THT HTH TTH HTT TTT Sample space
[Page 6]
Probability  Use the previous sample space of two rolls of a four sided dice: Ï‰ =(Ï‰1,Ï‰2), Ï‰i = value of the ith toss The following functions are all random variables: R = outcome of the first toss, R(Ï‰) = Ï‰1 S = outcome of the second toss, S(Ï‰) = Ï‰2 X = product of two faces, X(Ï‰) = Ï‰1 Ã— Ï‰2 Y = (difference)2, Y(Ï‰) = (Ï‰1 - Ï‰2)2 Examples
[Page 7]
Probability  ï¬A function of a random variable defines another random variable Since a random variable (RV) is a just functionã€‚å†å–ä¸€æ¬¡å‡½æ•¸ï¼Œå‰‡å…¶å®šç¾©åŸŸç‚ºåŸæ¨£æœ¬ç©ºé–“ï¼Œå€¼ä»ç‚ºä¸€å¯¦æ•¸ ä¸‹åœ–ä¾‹: X æ˜¯ä¸€å€‹éš¨æ©Ÿè®Šæ•¸ï¼ŒY=g(X) ä¹Ÿæ˜¯ä¸€å€‹éš¨æ©Ÿè®Šæ•¸ï¼Œåªæ˜¯å°‡æ¨£æœ¬ç©ºé–“å…§å…ƒç´ å°æ‡‰åˆ°ä¸åŒçš„å€¼! ï¬ It is to the designerâ€™s convenience to define different RVs on a single experiment R and S in the previous example are the most straightforward We can use R and S to describe the other random variables, e.g., X=RS å°éš¨æ©Ÿè®Šæ•¸å–å‡½æ•¸é‹ç®—(Function of An RV)
[Page 8]
Probability  éš¨æ©Ÿè®Šæ•¸èˆ‡äº‹ä»¶ä¹‹é–“çš„é—œä¿‚ ï¬ When a random variable takes on a specific real value or belongs to certain set, it constitutes an event. Example 1: X=4 in the previous example (X=RS) means the event that the product of two rolls is 4 ï¬ For a random variable X and a specific value x, { X=x } is an event. Thus, we can find the probability associated with this event. Conditional probability and concept of independence starts to kick in with RVs: Ex: P(R=1|X=4)=? ï¬ æ›´å»£ç¾©è€Œè¨€ï¼Œä¸€å€‹æˆ–å¤šå€‹éš¨æ©Ÿè®Šæ•¸é–“çš„é‹ç®—(ç­‰å¼ã€ä¸ç­‰å¼ã€å±¬æ–¼)çµæœå‡æœƒå°æ‡‰åˆ°ä¸€å€‹äº‹ä»¶ Ex: (1) R + S= 6 æ‰€å°æ‡‰åˆ°çš„äº‹ä»¶ç‚ºä½•? (2) ğ‘…ğ‘…âˆˆ{2,4,6} è¡¨ç¤ºçµæœç‚ºå¶æ•¸çš„äº‹ä»¶ éš¨æ©Ÿè®Šæ•¸èˆ‡äº‹ä»¶ é€™æ˜¯ä¸€å€‹äº‹ä»¶
[Page 9]
Probability  éš¨æ©Ÿè®Šæ•¸å¯åˆ†ç‚ºå…©é¡: é›¢æ•£èˆ‡é€£çºŒ ï¬ Discrete random variables (é›¢æ•£éš¨æ©Ÿè®Šæ•¸, Chapter 2) Discrete random variables are defined over discrete experiments Ex 1: ä¸ŸéŠ…æ¿: X(æ­£é¢)=1ã€X(åé¢)=0 Ex 2: ä¸‹æ£‹æ¯”è³½: X(Type 1å°æ‰‹)=0ã€X(Type 2å°æ‰‹)=1ã€X(Type 3å°æ‰‹)=2 é›¢æ•£RVå€¼åŸŸè£¡çš„å€¼ä¸€å®šæ˜¯ã€Œå¯æ•¸çš„ã€ã€‚ å€‹æ•¸å¯ä»¥æ˜¯æœ‰é™å€‹ï¼Œæˆ–æ˜¯ã€Œå¯æ•¸çš„ã€ç„¡çª®å¤šå€‹(å¦‚æ•´æ•¸ä¸€èˆ¬)ã€‚ ï¬ Continuous random variables (é€£çºŒéš¨æ©Ÿè®Šæ•¸, Chapter 3) Ex 1: å¹¸é‹æ—‹è½‰ç›¤æŒ‡é‡æ‰€æŒ‡çµæœX å¯ä»¥æ˜¯0åˆ°1ä¹‹é–“çš„ä»»ä½•å¯¦æ•¸ Ex 2: ç¾…å¯†æ­èˆ‡èŒ±éº—è‘‰çš„é²åˆ°æ™‚é–“X å’ŒY å¯ä»¥æ˜¯0åˆ°1ä¹‹é–“çš„ä»»ä½•å¯¦æ•¸ Ex 3: é›·é”ç³»çµ±æ¥æ”¶åˆ°ç”±é£›æ©Ÿåå°„å›ä¾†çš„è¨Šè™ŸY å¯ä»¥æ˜¯ä»»æ„å¯¦æ•¸ Ex 4: æ‰‹æ©Ÿæ”¶åˆ°çš„è¨Šè™ŸY å¯ä»¥æ˜¯ä»»æ„å¯¦æ•¸ é€£çºŒRVå€¼åŸŸè£¡çš„å€¼ä¸€å®šæ˜¯ã€Œä¸å¯æ•¸çš„ã€ã€‚å€‹æ•¸ä¸€å®šæ˜¯ç„¡çª®å¤šå€‹(å¦‚å¯¦æ•¸ä¸€èˆ¬)ã€‚ éš¨æ©Ÿè®Šæ•¸çš„åˆ†é¡
[Page 10]
Probability  é›¢æ•£éš¨æ©Ÿè®Šæ•¸ ï¬é‡é»å›é¡§ éš¨æ©Ÿè®Šæ•¸çš„å®šç¾© ç‚ºä»€éº¼å¼•é€²éš¨æ©Ÿè®Šæ•¸çš„æ¦‚å¿µ? éš¨æ©Ÿè®Šæ•¸èˆ‡äº‹ä»¶çš„é€£çµ éš¨æ©Ÿè®Šæ•¸çš„åˆ†é¡ â†’é›¢æ•£éš¨æ©Ÿè®Šæ•¸ã€é€£çºŒéš¨æ©Ÿè®Šæ•¸ ï¬Next,å…ˆè«‡è«‡é›¢æ•£éš¨æ©Ÿè®Šæ•¸ Probability mass function ä¸€äº›å¸¸è¦‹çš„é›¢æ•£éš¨æ©Ÿè®Šæ•¸ â†’Uniform â†’Bernoulli å’ŒBinomial â†’Geometric â†’Poisson
[Page 11]
Probability  ï¬ A random variable is called discrete if its range is finite or at most countably infinite ï¬ A discrete random variable is characterized through the probabilities of the values it can take ï¬ Probability Mass Function (PMF) of a discrete RV X: æ©Ÿç‡è³ªé‡å‡½æ•¸ If x is any possible value of a discrete RV X, the probability mass of x, denoted pX(x), is the probability of the event {X = x} consisting of all outcomes that give rise to X equal to x: pX(x) = P({X = x}) = P({Ï‰: X(Ï‰) = x}) Ex: æŠ•æ“²å…¬æ­£éª°å­ï¼ŒX ç‚ºå…¶å‡ºç¾é»æ•¸pX(6)=? We will simply use P(X = x) to represent P({X = x}) æ©Ÿç‡è³ªé‡å‡½æ•¸å’Œé›¢æ•£éš¨æ©Ÿè®Šæ•¸æ˜¯ç¶åœ¨ä¸€èµ·çš„ Probability Mass Function (PMF) Probability of the event {X=x}
[Page 12]
Probability  Remarks: ï¬ Comments on the notation pX(x) = P(X = x) Subscript X is the random variable of interest, itâ€™s capital and italic The argument x is the real numerical value that the random variable X may take ï¬ With PMF, we can use the third axiom of probability (additivity) to compute the probability of any event involving the random variable P( ) = âˆ‘xâˆˆS pX (x) This is an event!
[Page 13]
Probability  General Rule of Finding PMF ï¬ Collect all the sample points in the original sample space that give rise to the mapping X=x for a fixed x. Find the set in the original sample space Sx={ Ï‰: X(Ï‰) = x } ï¬ Compute the probability of this set using the original probability measure pX(x) = P( { Ï‰: X(Ï‰) = x } ) =âˆ‘Ï‰âˆˆSx P( {Ï‰} ) ï¬If pX(x) is a PMF, then it must satisfy 0 â‰¦pX(x) â‰¦1 for all x âˆ‘x pX(x) = 1 ä¸Šè¿°å…©æ¢ä»¶å¯è¢«ç”¨ä¾†é©—è­‰ä¸€å€‹PMF æ˜¯å¦è¢«æ­£ç¢ºè¨ˆç®—å‡ºä¾†ï¼Œåˆä¹è¦ç¯„(legitimate)
[Page 14]
Probability  Example Let X be the number of heads obtained in two independent tosses of a fair coin. What is the probability of at least one head?
[Page 15]
Probability  Uniform PMF A uniform PMF puts equal probability on all sample points. Suppose a random variable takes on N possible values 0,1,â€¦,N-1, equally likely. Then, A uniform PMF provides a good model in experiments where there is no reason to suspect that any outcome is more or less likely than any other. Common examples: 1. Flipping a fair coin 2. Rolling a fair dice
[Page 16]
Probability  Bernoulli Random Variable: Let X take values only on {0,1} with P(X=1)=p. The probability mass function is called Bernoulli PMF and is given by Bernoulli çš„æ‡‰ç”¨ Bernoulli RV å¯ç”¨ä¾†æè¿°æŸä»¶äº‹æƒ…æˆåŠŸæˆ–å¤±æ•—ï¼Œé€™ç¨®å–®ç´”çš„äºŒå…ƒäº‹ä»¶ã€‚å¦‚: 1. éŠ…æ¿(æ­£é¢ã€åé¢): pX(æ­£é¢)=0.5 2. å­ç‘œæ˜¯å¦ç­”æ‡‰æ™šé¤é‚€ç´„: pX (ç­”æ‡‰)=0.3ã€pX (æ‹’çµ•)=0.7 3. ç„¡ç·šé€šè¨Šç¶²è·¯ä¸­è³‡æ–™å‚³è¼¸æˆåŠŸèˆ‡å¦: pX (æˆåŠŸ)=0.9ã€pX (å¤±æ•—)=0.1 Questions ? Is Bernoulli PMF a legitimate PMF? æª¢æŸ¥Is it nonnegative (obvious) ã€Does it sum to 1? Bernoulli PMFs
[Page 17]
Probability  Binomial PMF Binomial Random Variable: Let X = number of heads in n independent coin tosses. P(æ­£é¢) = p (bias of the coin) What is the PMF pX(x) of X ? Note that for x = 2, n = 4 pX(2) = P(HHTT) + P(HTHT) + P(HTTH) + P(THHT) + P(THTH) + P(TTHH) = 6p2(1-p)2 Binomial PMF In general, binomial X with parameters (n,p) has PMF
[Page 18]
Probability  Binomial PMF ï¬ Binomial çš„æ‡‰ç”¨ Binomial RV å¯ç”¨ä¾†æè¿°æŸä»¶äº‹åœ¨n æ¬¡çš„è©¦é©—ä¸­æˆåŠŸçš„æ¬¡æ•¸ï¼Œåƒæ•¸ç‚ºn å’Œpã€‚ä¾‹å¦‚: 1.é‚€ç´„å­ç‘œæ™šé¤10æ¬¡ä¸­æˆåŠŸ6æ¬¡çš„æ©Ÿç‡? (n=10, p=0.3) 2. è³‡æ–™å‚³100æ¬¡ä¸­æˆåŠŸ95æ¬¡çš„æ©Ÿç‡? (n=100, p=0.9) ï¬ We can describe a binomial random variable using Bernoulli random variables!! å‡è¨­ä»¥Bernoulli Xi è¡¨ç¤ºç¬¬i æ¬¡è©¦é©—çš„çµæœï¼ŒXi(æˆåŠŸ)=1 ï¼ŒXi(å¤±æ•—)=0ï¼Œä¸”P(Xi=1)=pã€‚å‰‡binomial RV Y with parameters (n,p) å¯è¡¨ç¤ºæˆ
[Page 19]
Probability  Geometric Random Variable Geometric Random Variable: Flip a coin with a bias p until you get the first head. The random variable X defined as the number of flips required is called geometric random variable. pX(1) = P(H1) = p pX(2) = P(T1H2) = (1-p) p pX(3) = P(T1T2H3) = (1-p)2p Geometric PMF: In general, geometric RV å¯ç”¨ä¾†æè¿°å˜—åˆ°ç¬¬ä¸€æ¬¡æˆåŠŸæ‰€éœ€è¦å¯¦é©—çš„æ¬¡æ•¸ã€‚ the PMF is given by pX(k) = (1-p)k-1p, for k=1, 2, 3, â€¦ Question: Is it a legitimate PMF?
[Page 20]
Probability  Poisson Random Variable** Poisson PMF: A discrete random variable X taking on nonnegative integers is called Poisson random variable with parameter Î» if its PMF is given by Questions: 1. Is it a legitimate PMF? 2. What are the applications of Poisson random variables?
[Page 21]
Probability  Poisson PMF æ€éº¼å¾—ä¾†çš„? Poisson is good approximation for binomial with n large, p small and np a moderate fixed value Î». Introduced by French mathematician Simeon-Denis Poisson in 1837 Consider a binomial random variable with (n, p) such that 1) nâ†’âˆ 2) pâ†’0 3) np=Î»
[Page 22]
Probability  Applications of Poisson Random Variables Poisson random variable is typically used to model the number of occurrences of rare events, e.g. â€” number of typos in a book â€” number of phone calls arriving in an interval of T seconds â€” number of customers being served by a bank teller in an hour â€” number of packets in a network received at a router in T seconds We will see more applications of Poisson in later chapters (Chapter 6).
[Page 23]
Probability  Example å‡è¨­æ¯ä½é€²å…¥å·¨åŸçš„é¡§å®¢æœ‰0.01çš„æ©Ÿæœƒæ˜¯é™½æ˜äº¤å¤§çš„å­¸ç”Ÿã€‚è«‹å•100ä½å®¢äººä¸­æ°æœ‰5ä½é™½æ˜äº¤å¤§å­¸ç”Ÿ çš„æ©Ÿç‡ç‚ºä½•? (Sol.) 1. From binomial 2. Approximated by Poisson ( ğœ†ğœ†= ğ‘›ğ‘›ğ‘›ğ‘›= 100 Ã— 0.01 = 1)
[Page 24]
Probability  Poisson çš„å¸¸ç”¨å‹ Poisson R.V. ğ‘‹ğ‘‹ğ‘¡ğ‘¡can often be used to describe the number of a certain event occurred within a time period 0, ğ‘¡ğ‘¡. This ğ‘‹ğ‘‹ğ‘¡ğ‘¡is called a Poisson process. The PMF of ğ‘‹ğ‘‹ğ‘¡ğ‘¡with parameter ğ›¼ğ›¼is given by Example: You get an email according to a Poisson process with parameter ğ›¼ğ›¼=0.2. What is the probability of finding 0 and 1 new email within an hour?
[Page 25]
Probability  Functions of Random Variables (Revisited) Function of a random variable is also a random variable. Consider a random variable X. We define a function Y = g (X) of the random variable X, such as X2,ï½œXï½œ, cos (X) , or e3x Then, as discussed earlier, these are also random variables defined on the original experiment by Y (Ï‰) = g ( X (Ï‰ )). å•é¡Œæ˜¯ï¼Œè‹¥å·²çŸ¥X çš„PMFï¼Œå¦‚ä½•æ±‚å¾—Y çš„PMFå‘¢?
[Page 26]
Probability  PMF of Functions of Random Variables Given the PMF of X, pX(x), the PMF of Y=g(X) is
[Page 27]
Probability  Example: Recall the random variable X = maximum of two rolls of a four-sided die. Define a new random variable Y = g (X) by Find the PMF for Y.