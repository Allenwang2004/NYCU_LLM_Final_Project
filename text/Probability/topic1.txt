
[Page 1]
Topic 1: Probabilistic Models Lecture Outline Basic probabilistic model: an experiment Review of Classical Probability (å¤å…¸æ©Ÿç‡) Sample space (æ¨£æœ¬ç©ºé–“) Events (äº‹ä»¶) Probability measure â†’Three axioms of probability (æ©Ÿç‡ä¸‰å¤§å…¬ç†) Total Probability Theorem (å…¨æ©Ÿç‡å®šç†) Reading: é«˜ä¸­æ•¸å­¸èª²æœ¬ã€Bertsekas & Tsitsiklis 1..2
[Page 2]
Review of Classical Probability è€ƒæ…®æœ‰ğ‘ğ‘å€‹å…ƒç´ çš„æ¨£æœ¬ç©ºé–“(sample space) ğ‘†ğ‘†ï¼Œä¸”å‡è¨­æ¯å€‹æ¨£æœ¬é»å‡ºç¾çš„æ©Ÿæœƒå‡ç­‰ã€‚S ä¸­çš„äº‹ä»¶ ğ´ğ´æœ‰ğ‘˜ğ‘˜å€‹å…ƒç´ ï¼Œå‰‡äº‹ä»¶A ç™¼ç”Ÿçš„æ©Ÿç‡å®šç¾©ç‚º P ğ´ğ´â‰œğ‘›ğ‘›(ğ´ğ´) ğ‘›ğ‘›(ğ‘†ğ‘†) = ğ‘˜ğ‘˜ ğ‘ğ‘ å…¶ä¸­ğ‘›ğ‘›(ğ´ğ´) è¡¨ç¤ºäº‹ä»¶ğ´ğ´ä¸­æ¨£æœ¬é»å€‹æ•¸ã€‚æ­¤å®šç¾©æ˜¯ç”±Laplace (æ³•åœ‹äººï¼Œ1749~1827)æ‰€æå‡ºï¼Œç¨±ç‚º å¤å…¸æ©Ÿç‡(classical probability)å®šç¾©æ³•ã€‚ Example: ä¸€å‰¯æ’²å…‹ç‰Œæœ‰52å¼µï¼Œå‡å‹»æ´—ç‰Œå¾Œä»»å–ä¸€å¼µï¼Œè‹¥æ¯å¼µè¢«å–å‡ºæ©Ÿæœƒç›¸ç­‰ï¼Œè©¦æ±‚å–å‡ºçš„ç‰Œæ˜¯é»‘æ¡ƒçš„æ©Ÿç‡ ç‚ºå¤šå°‘? ä»¥ä¸Šæ˜¯é«˜ä¸­æ•¸å­¸ã€‚
[Page 3]
Limitations of Classical Probability ï¬It cannot handle events with an infinite number of possible outcomes Ex: Possible value of received electro-magnetic wave in your smartphone ï¬It also cannot handle events where each outcome is not equally likely Ex: ä¸Ÿæ“²ä¸€é¡†ä¸å…¬æ­£çš„éª°å­ï¼Œå‡ºç¾å¶æ•¸çš„æ©Ÿç‡? These limitations make classical probability inapplicable for complicated tasks. We need something more!
[Page 4]
Components of a Probabilistic Model A probabilistic model is a mathematical description of a process (which is called experiment) with an outcome that is not fully predictable, e.g. a coin flip. Components: Outcome: each experiment produces exactly one outcome (Ex: éŠ…æ¿çš„æ­£é¢æˆ–åé¢). An outcome is also called a sample point (æ¨£æœ¬é») Sample space: A list of all outcomes of an experiment â†’A sample space is a set â†’Any subset of the sample space is called an event (äº‹ä»¶) Algebra of events (set theory): language for manipulating collections of elementary events
[Page 5]
Experiment Example: Three flips of a fair coin labeled 1 (or heads) on one side and 0 (or tails) on the other. Many ways to represent outcomes: ï¬ List or table: each column corresponds to the number of the flip 000 001 010 011 100 101 110 111 ï¬ Sequential description: a tree
[Page 6]
1 0 1 1 1 1 1 1 0 0 0 0 0 0 111 110 101 100 011 010 001 000
[Page 7]
Example: Roll a fair four-sided die twice Outcome of the experiment We can use a table or tree as above to represent all possible outcomes, or a graphical representation as below: 1 2 3 4 1 2 3 4 1st roll 2nd roll
[Page 8]
A sample space is a collection of all of the possible elementary outcomes (sample points) A sample space is an example of a set â†’Dealing with a sample space requires using basic set theory We define Sample points are all disjoint or mutually exclusive (äº’æ–¥), i.e., they are separate and distinct outcomes Sample points are collectively exhaustive, i.e., together they make up the entire sample space, they constitute all possible elementary outcomes We often use Î© to denote the sample space. A sample point Ï‰ in the sample space Î© can be described by Ï‰ âˆˆÎ© Sample Space (æ¨£æœ¬ç©ºé–“)
[Page 9]
Event (äº‹ä»¶) Definition Events are just subsets of the sample space, i.e., sets of elements which belong to Î© Examples of events: Event A: Flip three coins and get 010 (1 means Head, 0 means Tail) â†’This is a simple event (elementary event, which is just an outcome) â†’A={010} Event B: Flip three coins and get exactly one 1 â†’More complicated event, consisting of three elementary events 001, 010, and 100 â†’B={001, 010, 100} Event C: Flip three coins and get a result such that the sum of values is 2 â†’C={011,101, 110} Remarks: 1.ç¬¦è™Ÿä¸Šä½¿ç”¨å·¦å³å¤§æ‹¬è™Ÿä¾†è¡¨ç¤ºé›†åˆã€äº‹ä»¶ 2.ã€Œæ©Ÿç‡ã€çš„è¨ˆç®—å¿…å’Œã€Œäº‹ä»¶ã€æœ‰é—œã€‚æˆ‘å€‘å°ç‰¹æ®Šäº‹ä»¶æœƒç™¼ç”Ÿçš„æ©Ÿç‡æ„Ÿèˆˆè¶£
[Page 10]
We usually use capital letter to denote an event F, a subset of â„¦, and write F âŠ‚â„¦ A set F might have only one point (i.e. outcome) in it, e.g., F = {Ï‰} for a specific Ï‰âˆˆâ„¦ â†’An outcome is itself an event (elementary event) All sets are subsets of themselves â†’the entire sample space Î© is an event A set might have no points in it, i.e., the empty set ğœ™ğœ™ â†’This is the event that "nothing happens" Events
[Page 11]
ï¬Use algebra of events, i.e., set theory, to manipulate events Combine simple events into complicated events Decompose complicated events into simpler events Perform operations, e.g. complement, intersection, and union on sets ï¬Three basic operations on sets Complement: FC ={Ï‰: Ï‰âˆ‰F} è£œé›†(æˆ–ç¨±é¤˜é›†) ïƒ All the sample points of â„¦that are not in F Intersection: Fâˆ©G={Ï‰: Ï‰âˆˆF and Ï‰âˆˆG} äº¤é›† â†’the points that are in both sets â†’If F and G have no points in common, then they are said to be disjoint or mutually exclusive, i.e. Fâˆ©G=Î¦, the null set (ç©ºé›†åˆ) Union: FâˆªG={Ï‰: Ï‰âˆˆF or Ï‰âˆˆG} è¯é›† â†’the points that are either in one set or the other, or both ï¬The definitions above can be illustrated by Venn diagrams Events
[Page 12]

[Page 13]
Set Theory Basics ï¬Fundamental set relations: FâˆªG =G âˆªF commutative law Fâˆª(G âˆªH) = (FâˆªG)âˆªH associative law Fâˆ©(G âˆªH) = (Fâˆ©G)âˆª(Fâˆ©G) distributive law (FC)C=F FCâˆ©F= ğœ™ğœ™(ç©ºé›†åˆ) (Fâˆ©G)C= FCâˆªGC DeMorganâ€™s law Fâˆ©â„¦=F ï¬Other relations: (F âˆª G)C= FC âˆ© GC DeMorganâ€™s other law
[Page 14]
Set Theory ï¬Other relations: F âˆ© G =G âˆ©F commutative law F âˆ©(G âˆ© H) = (F âˆ© G) âˆ© H associative law F âˆª(G âˆ© H) = (F âˆª G) âˆ©(F âˆª G) distributive law FC âˆª F= â„¦ F âˆª ğœ™ğœ™=F (ğœ™ğœ™ç©ºé›†åˆ) F âˆª(Fâˆ©G)=F=F âˆ©(F âˆª G) Fâˆª â„¦= â„¦ F âˆ© ğœ™ğœ™= ğœ™ğœ™ F âˆª G = F âˆª(FC âˆ© G) = F âˆª(G-F) ïƒ ä¹‹å¾Œæœƒç”¨åˆ°æ­¤ç‰¹åˆ¥çš„é›†åˆåˆ†å‰² F âˆª F =F F âˆ© F =F
[Page 15]
Probability Law We will want to assign probabilities to particular events occurring as a result of an experiment â†’Probability law The probability law, which assigns a nonnegative number P(A) to event A that encodes our knowledge or belief about the collective â€œlikelihoodâ€ of the elements of A, formally called the probability of A
[Page 16]
Probability Probability Law An assignment of a real number P(F) to every event F such that the following three simple axioms are satisfied: Non-negativity: P(F) â‰§0 for all event F Normalization: P(Î©) = 1 Additivity: If F and G are disjoint (Fâˆ©G= ğœ™ğœ™), then The function P(.) defined for all subsets of Î© is called a probability measure if and only if it satisfies the three axioms of probability ( ) ( ) ( ) P F G P F P G = + ï•
[Page 17]
Intuitions behind the probability axioms: Mimic relative frequencies (ç›¸å°ç™¼ç”Ÿæ¬¡æ•¸), i.e., how frequent an event occurs. For example, perform a sequence of N experiments (such as roll a dice N times): The relative frequency of an event F is Relative frequency is nonnegative (Non-negativity) The relative frequency of something happening is 1 (Normalization) Relative frequency of the union of two disjoint events add (Additivity) number of times F occurs N Intuition for Probability Axioms
[Page 18]
Elementary Properties Assume that P(.) is a probability measure defined on a sample space Î©. Then, we can use the three axioms to prove the following properties: 1. P(FC) = 1 âˆ’P(F) 2. P(F) â‰¤1 3. P(ğœ™ğœ™) = 0 4. If an event F is the union of a finite collection of disjoint events {Fi; i=1,â€¦,n}, i.e., if Fi âˆ© Fk = ğœ™ğœ™(ç©ºé›†åˆ) for iâ‰ k, and ğ¹ğ¹= â‹ƒğ‘–ğ‘–=1 ğ‘›ğ‘› ğ¹ğ¹ğ‘–ğ‘–, then 1 ( ) ( ) n i i P F P F = =âˆ‘
[Page 19]
5. Total Probability Theorem: (å…¨æ©Ÿç‡å®šç†ã€åŠ æ³•æ³•å‰‡) If { Fi; i=1,2,â€¦,K } is a finite partition of Î©, i.e., if Fi âˆ© Fk= ğœ™ğœ™ï¼Œwhen iâ‰ k, and F1 âˆªF2â€¦ âˆª FK= Î©, then for any event G, we have é€™æ˜¯ä¸€å€‹éå¸¸é‡è¦çš„çµæœ! æˆ‘å€‘å¯é€éå°‡Gæ‹†è§£ï¼Œä¾†è¨ˆç®—è¤‡é›œäº‹ä»¶Gçš„æ©Ÿç‡å€¼ã€‚ Example A fair coin is flipped twice. Let G be the event that the first flip results in Heads. What is the probability of event G? 1 ( ) ( ) K i i P G P G F = =âˆ‘ ï‰ Elementary Properties
[Page 20]
Total Probability Example Example: ä¸€æ£Ÿæœ‰40ä½æˆ¶çš„å¤§æ¨“åªæœ‰30å€‹è»Šä½ï¼Œæ¯å¹´å¿…é ˆé€éæŠ½ç±¤æ–¹å¼æ±ºå®šå“ªä¸€ç”¨æˆ¶æ–¼è©²å¹´åº¦æœ‰è»Šä½å¯åœã€‚ æŠ½ç±¤æ–¹å¼å¦‚ä¸‹: å°‡1~40è™Ÿçš„è™Ÿç¢¼çƒæ”¾ç½®ä¸é€æ˜ç®±ä¸­ï¼Œæ¯æˆ¶è¼ªæµæŠ½å–ä¸€çƒï¼Œå–å¾Œä¸æ”¾å›ã€‚æŠ½å‡º1~30è™Ÿçš„ä½æˆ¶å¯å¾—è»Š ä½ä½¿ç”¨æ¬Šã€‚ è«‹å•ï¼Œå…ˆæŠ½æœ‰åˆ©é‚„æ˜¯å¾ŒæŠ½?
[Page 21]
Elementary Properties 6. If FâŠ‚G, then P(F)â‰¤P(G) 7. P( FâˆªG )= P(F) + P(G) - P(F âˆ© G) 8. P( FâˆªG ) â‰¤P(F) + P(G) (Union bound or Bonferroni inequality)
[Page 22]
The fourth property provides explanations to classical probability, which youâ€™ve learned in high school, using counting arguments (æ’åˆ—çµ„åˆ) Consider a sample space with a finite number of sample points Since sample points are by definition disjoint ( {ğœ”ğœ”ğ‘–ğ‘–} âˆ©{ğœ”ğœ”ğ‘—ğ‘—} = ğœ™ğœ™for ğ‘–ğ‘–â‰ ğ‘—ğ‘—), it follows from the fourth property that for any event F We can find the probability of an event with finite sample points by adding up the probabilities of all the sample points in the event â†’Need to know how many such kâ€™s (# of sample points) â†’Need to know P({Ï‰k}) for each k This is great for computing probabilities for discrete experiments (coins, dice, etc.), but not for continuous experiments! (Why not?) 1 2 { , , , } N Ï‰ Ï‰ Ï‰ â„¦= ï‹ : ( ) ({ }) k k k F P F P Ï‰ Ï‰ âˆˆ = âˆ‘ Discrete Probability Law
[Page 23]
Discrete Uniform Probability Law If all sample points are equally probable, i.e., and This implies We can compute probabilities by counting for experiments with finite sample spaces. This is called discrete uniform probability law, equivalent to those given by å¤å…¸æ©Ÿç‡è«– Examples: â†’Probability of getting exactly one 1 in three coin flips: â†’Probability get an odd number of 1's in three coin flips: 3 8 1 ({ }) ; 1,2, , k P k N N Ï‰ = = ï‹ number of sample points in F ( ) total number of sample points P F = 4 1 8 2 = Example 1 2 { , , , } N Ï‰ Ï‰ Ï‰ â„¦= ï‹
[Page 24]
Consider countably infinite sample space, such as the set of all integers Uniform law does not make sense, since the sum will go to infinity Nonuniform law is more reasonable with the additivity property We need a stronger form of Axiom 3: countable additivity, limiting form of Axiom 3 If F1,F2,F3â€¦ are disjoint, then Example: Suppose the sample space is {1,2,3,â€¦}, which is countable but infinite, and ğ‘ƒğ‘ƒğ‘›ğ‘›= 2âˆ’ğ‘›ğ‘›. Pr(outcome is even) = P({2,4,6,8, }) = P(2)+P(4)+P(6)+ ï‹ ï‹ What if discrete, but not finite? 1 2 1 (2 ) 1 (2 ) 3 k k k P k P âˆ = âˆ âˆ’ = = = = âˆ‘ âˆ‘ âˆ‘ âˆ = âˆ = = ï£·ï£· ï£¸ ï£¶ ï£¬ï£¬ ï£­ ï£« 1 1 ) ( k k k k F P F P ï•
[Page 25]
What if continuous sample space, i.e., uncountably infinite outcomes? For continuous sample space, since the outcomes are not countable, the idea of adding up probabilities of elementary points does NOT work! â†’Calculus comes into play ( sum ïƒ integral ) Examples: 1. A wheel of fortune is continuously calibrated from 0 to 1, so the possible outcomes of an experiment consisting of a single spin are the numbers in the interval Î©= [0, 1]. 2. What is the probability of the event consisting of a single element? Continuous Probability Law
[Page 26]
Romeo and Juliet will have a date. Each arrives late with a random delay of up to 1 hour, where the pair of delays is equivalent to that achievable by spinning two identical fair wheels. Each will wait only 1/4 of an hour before leaving. What is the probability that Romeo and Juliet will meet? Crosshatched region Answer: 1 {( , ) : } 4 x y x y = âˆ’ â‰¤ 2 Area of crosshatched region 1 1 2 (.75) .4375 Area of sample space 2 = âˆ’Ã— = Example (Ex. 1.5)