
[Page 1]
1 Topic 7: Multiple Continuous Random Variables Lecture Outline ï¬ Conditioning on an Event ï¬ Joint Probability Density Functions ï¬ Conditioning on a Random Variable  æ¢ä»¶æ©Ÿç‡Conditional probability conditioned on a continuous RV  æ¢ä»¶æœŸæœ›å€¼Conditional expectation  æ¢ä»¶æ©Ÿç‡å¯†åº¦å‡½æ•¸Conditional PDF ï¬ Continuous Bayes Rule ï¬ Variants of Total Probability Theorem (total probability theorem çš„å››å€‹è®Šå½¢) Reading : Textbook 3.4- 3.6
[Page 2]
2 Multiple Random Variables Quite naturally, we definitely will have to deal with multiple RVs in a real-world problem: ï¬ The total amount of time we need to wait in the line at the supermarket counter ï¬ What information can the multiple antennas of a WiFi router provide to the receiver? Again, we are particularly interested in the following: ï¬ How different random variables are related to each other? ä¸åŒéš¨æ©Ÿè®Šæ•¸é–“ä¹‹é—œè¯æ€§ (æ­£ç›¸é—œã€è² ç›¸é—œã€é›¶ç›¸é—œã€ç¨ç«‹ã€æ¢ä»¶æ©Ÿç‡ã€æ¢ä»¶æœŸæœ›å€¼ã€æ¢ä»¶æ©Ÿç‡å¯†åº¦å‡½æ•¸) ï¬ How to learn the behavior of the sum of all RVs? Ex: åˆ†æç¶²è·¯ç³»çµ±ä¸­è³‡æ–™æµé‡é€²å‡ºæŸç¶²è·¯ç¯€é»æ‰€è€—è²»çš„æ™‚é–“å»¶é²(æ’éšŠç†è«–, queueing) ï¬ How to learn the behavior or the true value of one RV when it is buried in the sum with other random variables? Ex: å¦‚ä½•å¾æ¥æ”¶è¨Šè™Ÿ(Y)=å‚³é€è¨Šè™Ÿ(S)+é›œè¨Š(N) ä¸­æ“·å–S çš„è³‡è¨Šã€‚å…¶ä¸­é›œè¨Šä¸€èˆ¬æ˜¯ continuous RVï¼Œåœ¨æ•¸ä½é€šè¨Šçš„æŠ€è¡“ä¸‹ï¼Œå‚³é€è¨Šè™ŸS æ˜¯discreteã€‚æ¥æ”¶è¨Šè™ŸY ç‚ºé€£çºŒã€‚
[Page 3]
3 Conditioning on an Event Definition Similar to the definition of unconditional PDF, the conditional PDF of a continuous random variable X, conditioned on a particular event E with P(E) > 0, is a function fX|E that satisfies (1) Special Case Conditioning on the event E that X belongs to a subset A of the real line, we have (2) Therefore, comparing (2) with (1), we have
[Page 4]
4 Illustration of Conditioning on an Event Special Case Conditioning on the set when X belongs to a subset A of the real line, we have We see itâ€™s just a scaling of fx(x). So the PDF within the conditioning set has the same shape as the unconditional PDF
[Page 5]
5 Example: The Exponential Random Variable Is Memoryless Example 3.13 (Memoryless Property of Exponential) The time T until a new light bulb burns out can be modeled by an exponential random variable with parameter Î». Alice turns the light on, leaves the room, and when she returns, t time units later, finds that the light bulb is still on, which corresponds to the event A={T>t}. Let X be the additional time until the light bulb burns out. What is the conditional CDF of X, given the event A?
[Page 6]
6 Total Probability Theorem Using Conditional PDF** If A1,A2, . . . , An are disjoint events with P(Ai) > 0 for each i, that form a partition of the sample space, then é€™æ˜¯å¸¸ç”¨çš„æŠ€å·§ï¼Œå¯å”åŠ©è¨ˆç®—å‡ºè¤‡é›œX çš„PDFã€‚ How to justify the equation? See Example 3.14 in the textbook Another example: Let X be a standard normal random variable. A new random variable Y is defined as follows: We flip a coin. If the outcome is a head, then Y= X. And if the outcome of the coin flip is a tail, then ğ‘Œğ‘Œ= âˆ’ğ‘‹ğ‘‹. Assume the coin flip is independent with X. Please find the PDF of Y.
[Page 7]
7 Total Probability Theorem using Conditional PDF (Example 3.14) The metro train arrives at the station near your home every quarter hour starting at 6:00 a.m. You walk into the station every morning between 7:10 and 7:30 a.m. and your arrival time is a uniform random variable over this interval. What is the PDF of the time you have to wait for the first train to arrive?
[Page 8]
8 Total Expectation Theorem using Conditional Expectation Definition The conditional expectation is defined by Total Expectation Theorem If A1,A2, . . . , An are disjoint events with P(Ai) > 0 for each i, that form a partition of the sample space, then See Example 3.17 in textbook.
[Page 9]
9 Total Expectation Theorem using Conditional Expectation (Example 3.17) Suppose that the random variable X has the piecewise constant PDF Find E[X] and var(X).
[Page 10]
10 Joint Probability Density Function We say that two continuous random variables associated with a common experiment are jointly continuous and can be described in terms of a joint PDF fX,Y , if fX,Y is a nonnegative function that satisfies for every subset B of the 2-dimensional plane. The probability that (X,Y) falls within a rectangle is
[Page 11]
11 Joint Probability Density Function ï¬ PDF fX,Y(x,y) must satisfy normalization equation, i.e. ï¬ To interpret the PDF, we let Î´ be very small and consider the probability of a small rectangle. We have we can view fX,Y (a, c) as the â€œprobability per unit areaâ€ in the vicinity of (a, c)
[Page 12]
12 Marginal PDF from Joint PDF The marginal PDFs fX(x) and fY(y) of continuous X and Y are given by Recall that for discrete random RVs, we have æœ¬è³ªä¸Šï¼Œä¸Šè¿°è«¸å¼å‡æºè‡ªæ–¼total probability theorem
[Page 13]
13 Expectation ï¬ If X and Y are jointly continuous random variables, and g is some function, then Z = g(X, Y) is also a random variable. the expected value rule is ï¬ Using the expected value rule, for any scalars a, b, we have
[Page 14]
14 Conditioning on a Random Variable Definition Let X and Y be continuous random variables with joint PDF fX,Y. For any fixed y with fY (y) > 0, the conditional PDF of X given that Y = y, is defined by 1. Analogous to the formula in the discrete PMF case 2. fX|Y(x|y) is a legitimate PDF (check normalization) See example 3.19 in the textbook.
[Page 15]
15 Interpretation of Conditioning on a Random Variable Interpretation Fix some small positive numbers Î´1 and Î´2, and condition on the event B = {y â‰¤ Y â‰¤ y + Î´2}. We have We can think of the limiting case where Î´2 decreases to zero and write More generally,
[Page 16]
16 Conditional Expectation Definition The conditional expectation is defined by ï¬ The expected value rule applies to conditional expectation ï¬ Total expectation theorem
[Page 17]
17 Conditional Expectation Example The joint PDF of X and Y is Find the conditional expectation E[X|Y=y].
[Page 18]
18 Total Expectation Theorem Total expectation theorem for continuous random variable  Consider any event A. Let X be the random variable that takes the value 1 if event A occurs and the value 0 otherwise (Such RV is called an indicator). In this case, we have E[X]=P(A) . Then, we have the following version of total probability theorem, when conditioned on continuous random variable Y (See page 30 of this topic)
[Page 19]
19 Independence Two continuous random variables X and Y are independent if their joint PDF for all x and y is the product of the marginal PDFs: Suppose X and Y are independent. ï¬ Similar to the discrete case, the random variables g(X) and h(Y) are independent, for any functions g and h ï¬ We have or more generally, ï¬ IF X and Y are independent, we have
[Page 20]
20 Continuous Bayesâ€™ Rule The conditional PDF fX|Y(x|y) can by obtained via
[Page 21]
21 Statistical Inference using Bayesâ€™ Rule ä½¿ç”¨è²æ°å®šç†é€²è¡Œé æ¸¬ã€æ¨è«– ï¬ Unobserved phenomenon (cause) describes something we want to know about. But we can only observe a certain result (effect) at the receiving end. For example,  Unknown transmitted bit (0 or 1) in a communication system  Unknown presence of an airplane in a radar system  Unknown disease of a patient ï¬ Objective: (è—‰ç”±è§€å¯ŸY çš„å€¼ä¾†æ¨è«–X æˆ–N çš„å€¼ã€‚æ­¤è™•Yæ˜¯è’é›†ç²å¾—ä¹‹è³‡æ–™ã€è¨Šè™Ÿ) Determine which cause among many candidates is the most likely, by checking the posterior probability PN|Y(n|y) for all possible n (or posterior density fX|Y(x|y) ) Measurement Inference Transmission Medium Unobserved phenomenon Case 1: Continuous X Case 2: Discrete N Case 1: Continuous fX|Y(x|y) Case 2: Discrete PN|Y(n|y) Observed (Measured, Received) Typically continuous Y
[Page 22]
22 Statistical Inference using Bayesâ€™ Rule 1. Continuous case: The unobserved phenomenon is a continuous random variable X, the posterior density of the unknown X given observed Y=y is 2. Discrete case: The unobserved phenomenon is a discrete random variable N, the posterior probability of N given the observed Y=y is Measurement Inference Transmission Medium Unobserved phenomenon Observed (Measured, Received) Typically continuous, due to noise in the observation Y Case 1: Continuous X Case 2: Discrete N Case 1: Continuous fX|Y(x|y) Case 2: Discrete PN|Y(n|y)
[Page 23]
23 Example â€“ Signal Detection in Communication Systems** A binary signal S is transmitted, and we are given that P(S = 1) = p and P(S = âˆ’1) = 1âˆ’p. The received signal is Y = N+S, where N is normal noise, with zero mean and variance Ïƒ2, independent of S. What is the probability that S = 1, given that we have observed Y=y ?
[Page 24]
24 Joint CDF If X and Y are two random variables associated with the same experiment, we define their joint CDF by ï¬ The joint CDF ï¬ The PDF can be recovered from the PDF by differentiating:
[Page 25]
25 Joint PDF of Independent Gaussian RVs Let X and Y be two independent Gaussian random variables with means Î¼x ,Î¼y, and variances Ïƒx 2, Ïƒy 2 respectively. The joint PDF is given by ï¬ The joint PDF has a bell shape centered at (Î¼x ,Î¼y). Whether the PDF is a tall-thin bell or wide-fat bell is determined by the variances Ïƒx 2, Ïƒy 2 ï¬ Very often we are interested in knowing the contours (ç­‰é«˜ç·šåœ–ã€åˆ‡é¢åœ–) of the joint PDF. The contours are sets of points at which the PDF takes a constant value.
[Page 26]
Summary of Total Probability Theorem and Its Variants ï¬We have already learned several different versions of total probability theorem. The original version is given in the form of probability of events (Section 1.4, page 28) ï¬According to the types of underlying RVs in forming G and Fi, the TPT can be further generalized to the following variants: Variant 1: Discrete RV in G â€“ Discrete RV in Fi (DD Type) Variant 2: Continuous RV in G â€“ Continuous RV in Fi (CC Type) Variant 3: Continuous RV in G â€“ Discrete RV in Fi (CD Type) Variant 4: Discrete RV in G â€“ Continuous RV in Fi (DC Type) ï¬é€šå‰‡: When continuous RV is involved, the sum and PMF in original TPT need to be modified to integral and PDF 26 1 ( ) ( | ) ( ) K i i i P G P G F P F = =âˆ‘
[Page 27]
ç¬¬ä¸€é¡è®Šå½¢: Discrete RV in G â€“ Discrete RV in Fi Variant 1: Discrete RV in G â€“ Discrete RV in Fi (DD Type) ïƒ¼This follows directly from original version by setting G={X=x} and Fi ={Y=yi} ïƒ¼See Example 2.14 and 2.15. ïƒ¼Another possible forms: ïƒ¼Example: Let ğ‘‹ğ‘‹, ğ‘Œğ‘Œbe discrete RVs. Compute the probability ğ‘ƒğ‘ƒğ‘‹ğ‘‹+ ğ‘Œğ‘Œ= ğ‘¤ğ‘¤. 27
[Page 28]
ç¬¬äºŒé¡: Continuous RV in G â€“ Continuous RV in Fi Variant 2: Continuous RV in G â€“ Continuous RV in Fi (CCType) ïƒ¼See page 172 ïƒ¼Example 3.19, on page 179 (continuous Bayes rule) 28
[Page 29]
ç¬¬ä¸‰é¡: Continuous RV in G â€“ Discrete RV in Fi Variant 3: Continuous RV in G â€“ Discrete RV in Fi (CDType) ïƒ¼See page 180 and Example 3.20. ïƒ¼A more general form is (see page 167) ïƒ¼Example 3.14, on page 168. ïƒ¼See the example on page 6 of this topic. 29
[Page 30]
ç¬¬å››é¡: Discrete RV in G â€“ Continuous RV in Fi Variant 4: Discrete RV in G â€“ Continuous RV in Fi (CDType) ïƒ¼See page 181 and page 182. ïƒ¼A more general form is (page 181 of textbook, page 18 of this topic) ïƒ¼You can justify this relation by taking the integral over y to the 1st equation on page 181 of the textbook ïƒ¼Example: Let ğ‘‹ğ‘‹, ğ‘Œğ‘Œbe indepdent continuous RVs. Compute the PDF of X+Y. 30