
[Page 1]
Topic 2: Conditional Probability and Independence Lecture Outline ï¬ Conditional Probability (æ¢ä»¶æ©Ÿç‡) Chain Rule ï¬ Independence (ç¨ç«‹äº‹ä»¶) ï¬ Total Probability Theorem (å…¨æ©Ÿç‡å®šç†) ï¬ Bayes Rule (è²æ°å®šç†) Reading: Textbook 1.3, 1.4, 1.5
[Page 2]
Questions: ï¬ In an experiment involving two successive rolls of a die, you are told that the sum of the two rolls is 9. How likely is it that the first roll was a 6? ï¬ How likely is it that a person has a disease given that a medical test was negative? ï¬ A spot shows up on a radar screen. How likely is it that it corresponds to an enemy aircraft? æˆ‘å€‘æ‰€é¢å°çš„å·¥ç¨‹ç³»çµ±è¨­è¨ˆå•é¡Œï¼Œæˆ–æ˜¯é€éäººå·¥æ™ºæ…§è™•ç†çš„é ä¼°èˆ‡åˆ†é¡å•é¡Œå¹¾ä¹éƒ½æ˜¯åœ¨ å¾—çŸ¥æŸäº›äº‹ä»¶(å¦‚çµ¦å®šæŸäº›é‡æ¸¬è¨Šè™Ÿã€è³‡æ–™)çš„å‰æä¸‹ä½œå‡ºæ±ºç­–åˆ¤æ–·ã€‚é€™æ‰€çµ¦å®šçš„è¨Šè™Ÿã€ è³‡æ–™å³æ˜¯æ¢ä»¶æ©Ÿç‡ä¸­çš„çµ¦å®šâ€œæ¢ä»¶â€ã€‚ åœ¨å¾—çŸ¥å…¶ä»–æŸäº›äº‹æƒ…ç™¼ç”Ÿå¾Œï¼Œæˆ‘å€‘å°æ„Ÿèˆˆè¶£äº‹ä»¶çš„äº†è§£å¯èƒ½æœƒæœ‰æ‰€æ”¹è®Š(åŸæ©Ÿç‡vs.æ¢ä»¶æ©Ÿç‡)ã€‚ ç³»çµ±å†æ ¹æ“šè¨ˆç®—å‡ºçš„æ¢ä»¶æ©Ÿç‡å€¼åšå‡ºå°æ‡‰æ±ºç­–ã€‚ Motivations
[Page 3]
Example: æ©Ÿç‡æœŸä¸­è€ƒæ™‚ï¼Œæ··å“¥å°æŸå–®é¸é¸æ“‡é¡Œ4é¸é …Aã€Bã€Cã€Dæ¯«ç„¡é ­ç·’ã€‚æ²’å”¸æ›¸çš„æ··å“¥äº‚ çŒœç­”å°æ©Ÿç‡ç‚º1/4ã€‚ ç¢°å·§æ··å“¥åå·å§Šéš”å£ï¼Œå·å·ç„è¦‹ åœ¨æ­¤äº‹ä»¶ç™¼ç”Ÿå¾Œï¼Œå°æ··å“¥è€Œè¨€çŒœå°æ©Ÿç‡ç‚ºï¼Ÿ Example
[Page 4]
Letâ€™s look at a simple example before introducing the formal definition. (Example) Suppose that all six possible outcomes of a fair die roll are equally likely. If we are told that the outcome is even, we are left with only three possible outcomes, namely, 2, 4, and 6. What is the probability that the outcome is 6? Then, it is reasonable to let (æ ¹æ“šç›´è¦ºå¾—å‡ºçš„ç­”æ¡ˆ) P ( the outcome is 6 | the outcome is even ) = 1/3 Thus, an intuitive definition for conditional probability from the example is: Another Example # of outcomes in both and ( | ) # of outcomes in F G P F G G = Event F (æ„Ÿèˆˆè¶£ä¹‹äº‹ä»¶) Event G (è§€å¯Ÿåˆ°çš„ã€å·²çŸ¥äº‹ä»¶) = ğ‘›ğ‘›(ğ¹ğ¹âˆ©ğºğº) ğ‘›ğ‘›(ğºğº) = ğ‘›ğ‘›(ğ¹ğ¹âˆ©ğºğº)/ğ‘ğ‘ ğ‘›ğ‘›(ğºğº)/ğ‘ğ‘
[Page 5]
Conditional Probability Notation: Probability of F given G occurred. G becomes the new sample space. è§€å¯Ÿåˆ°G ä¹‹å¾Œï¼Œå°F æœ‰äº†æ–°çš„èªè­˜ï¼Œä¸åœ¨G äº‹ä»¶è£¡çš„outcome éƒ½ä¸æœƒç™¼ç”Ÿäº†ã€‚ Definition: Assume P(G)â‰ 0, the conditional probability is defined by If all elementary outcomes are equally likely (å¤å…¸æ©Ÿç‡), then we have ( | ) P F G ( ) ( | ) ( ) P F G P F G P G âˆ© = G F # of outcomes in both and ( | ) # of outcomes in F G P F G G =
[Page 6]
ï¬Conditional probabilities form a legitimate probability law We can prove that conditional probabilities satisfy the 3 axioms of probability theory, i.e. P(F|G) is nonnegative P(â„¦|G)=1 P(F1 ï•F2|G)=P(F1|G)+P(F2|G) for mutually exclusive F1 and F2 Conditional Probabilities Specify a Probability Law
[Page 7]
Chain Rule Motivation: It is often convenient to calculate unconditional probabilities (of the intersection of events) using conditional probabilities Chain Rule (ä¹˜æ³•æ³•å‰‡) Itâ€™s just a restatement of definition of conditional probability Multiplication rule (chain rule): å¯ä»¥å»¶ä¼¸åˆ° ( ) ( | ) ( ) P F G P F G P G âˆ© =
[Page 8]
Sequential Calculation of Probabilities Example: Three cards are drawn (without replacement) from an ordinary deck. What is the probability of not drawing any heart? Let ğ´ğ´ğ‘–ğ‘–= {æŠ½å‡ºçš„ç¬¬ğ‘–ğ‘–å¼µä¸æ˜¯ç´…å¿ƒ} i=1,2,3 ğ‘ƒğ‘ƒ(ğ´ğ´1âˆ©ğ´ğ´2 âˆ©ğ´ğ´3) = ğ‘ƒğ‘ƒ(ğ´ğ´1)ğ‘ƒğ‘ƒ(ğ´ğ´2 ğ´ğ´1 ğ‘ƒğ‘ƒ(ğ´ğ´3|ğ´ğ´1 âˆ©ğ´ğ´2)
[Page 9]
If an aircraft is present in a certain area, a radar detects and generates an alarm signal with probability 0.99. If it is not present, the radar generates an alarm with probability 0.10. We assume that an aircraft is present with probability 0.05. 1. What is the probability of no aircraft presence and false alarm? 2. What is the probability of aircraft presence and no detection? Mathematical Formulation: ï¬ G: Aircraft is flying above ï¬ F: The radar generates an alarm Equivalent to finding the probability of 1. missed detection? P(Gâˆ©Fc) 2. false alarm? P(Fâˆ©GC) Radar Detection Example
[Page 11]
More on Radar Detection: 1. Both cases can be interpreted as error probabilities since the signal processor (the radar) makes a mistake (either missed detection or false alarm) 2. Two types of error can have significantly different effects or costs  False alarm: not comfortable, but we can survive (=false positive å‡è­¦å ±ã€å½é™½æ€§)  Missed detection: not tolerable (=false negative, å½é™°æ€§) 3. Same issue arises in medical diagnoses, e.g., detecting cancer  False diagnosing a cancer (referred to as false alarm or false positive)  Missing a cancer that is there (missed detection or false negative)
[Page 12]
Independent Events (ç¨ç«‹äº‹ä»¶) If F and G are independent, then the occurrence of G should have no effect on the probability of F, that is, assuming ğ‘ƒğ‘ƒğºğºâ‰ 0 Or, equivalently, the above can be rewritten as Definition So, a formal definition for two events F and G to be independent is Remark The definition in (2) is more general than def. (1) since there is no requirement for ğ‘ƒğ‘ƒğºğºâ‰ 0. Nonetheless, def. (1) reveals the essence (physical interpretation, ç‰©ç†æ„ç¾©) of independence. (æ¢ä»¶æ©Ÿç‡=åŸæ©Ÿç‡ï¼Œè¡¨ç¤ºçµ¦ å®šçš„æ¢ä»¶ç„¡é—œç·Šè¦) ( | ) ( ) P F G P F = ( ) ( ) ( ) P F G P F P G âˆ© = (1) (2) ( ) ( ) ( ) P F G P F P G âˆ© =
[Page 13]
Question: Are disjoint events independent? Disjoint and Independent
[Page 14]
Definition: Given an event C with P(C)>0, the events A and B are called conditionally independent if P(Aâˆ©B |C) = P(A|C)P(B |C) Equivalent Condition: P(A|B âˆ©C) = P(A|C) Physical Interpretation: This relation states that if C is known to have occurred, the additional knowledge that B also occurred does not change the probability of A. In other words, the knowledge that B contains about A is totally included in C Conditional Independence
[Page 15]
Independence makes the sequential approach (multiplication rule) easier: Example: Independent tosses of a biased coin with P(Head)=p and P(Tail)=1-p
[Page 16]
In a sequential description of an experiment where a collection of outcomes are mutually independent, we say that the experiment is a sequence of independent trials. If the outcome is binary, we say we have a sequence of Bernoulli trials. Suppose we have a sequence of Bernoulli trials with outcomes ğ¹ğ¹1, ğ¹ğ¹2, â€¦ , ğ¹ğ¹ğ‘›ğ‘›such that Fi = { toss i of a biased coin is a head } with ğ‘ƒğ‘ƒğ»ğ»= ğ‘ğ‘. What is Pr( k heads in n tosses )? (or, in a more general language, of k successes in n Bernoulli trials) Note that for n = 4 and k = 2, 2 2 Pr( heads in tosses) ( ) ( ) ( ) ( ) ( ) ( ) 6 (1 ) k n P HHTT P HTHT P HTTH P THHT P THTH P TTHH p p = + + + + + = âˆ’ 2 2 4 (1 ) 2 p p ï£«ï£¶ = âˆ’ ï£¬ï£· ï£­ï£¸
[Page 17]
Binomial probability law: In general: are binomial coefficients Bernoulli probability law Bernoulli law is a special case of Binomial law in which n=1: - Pr( heads in tosses) (1 ) ; 0,1,2, , k n k n k n p p x n k ï£«ï£¶ = âˆ’ = â€¦ ï£¬ï£· ï£­ï£¸ 1- 1 Pr( heads in 1 toss) (1 ) 1 0 k k p k k p p p k = ï£± = âˆ’ = ï£²âˆ’ = ï£³
[Page 18]
Total Probability Theorem (TPT) Recall the elementary property of probability we called â€œtotal probability": If ğ¹ğ¹1, ğ¹ğ¹2, â€¦ , ğ¹ğ¹ğ¾ğ¾is a finite partition of Î©, i.e., ğ¹ğ¹ğ‘–ğ‘–âˆ©ğ¹ğ¹ğ‘˜ğ‘˜= ğœ™ğœ™when iâ‰ k and ï•i=1 Fi=â„¦, then If ğ‘ƒğ‘ƒ(ğ¹ğ¹ğ‘–ğ‘–) > 0 for all i, then using conditional probability we can write Key point: å°‡è¤‡é›œçš„äº‹ä»¶Gåˆ†è§£æˆè¼ƒç°¡å–®ä¸”å½¼æ­¤äº’æ–¥çš„äº‹ä»¶{ğºğºâˆ©ğ¹ğ¹1}ã€{ğºğºâˆ©ğ¹ğ¹2}ã€â€¦ã€{ğºğºâˆ©ğ¹ğ¹ğ¾ğ¾} ä¹‹è¯é›† å†æ¬¡æåŠï¼Œé€™æ˜¯ä¸€å€‹éå¸¸é‡è¦çš„çµæœ! 1. æˆ‘å€‘å¯é€éå°‡Gæ‹†è§£ï¼Œåˆ†é …è¨è«–ğ‘ƒğ‘ƒ(ğºğº|ğ¹ğ¹ğ‘–ğ‘–)å¾Œå†ç¶œåˆèµ·ä¾†è¨ˆç®—è¤‡é›œäº‹ä»¶Gçš„æ©Ÿç‡å€¼ã€‚ 2. å¾ŒçºŒè«‡åˆ°éš¨æ©Ÿè®Šæ•¸æ™‚æœƒå†æ¬¡ä½¿ç”¨TPTï¼Œè€Œæœƒæœ‰å››å€‹é‡è¦çš„è®Šå½¢! 1 ( ) ( ) K i i P G P G F = = âˆ© âˆ‘ 1 ( ) ( | ) ( ) K i i i P G P G F P F = =âˆ‘
[Page 19]
Example ï¬Chess tournament, 3 types of opponents - P(Type 1) = 0.5, P(Win|Type 1) = 0.3 - P(Type 2) = 0.25, P(Win|Type 2) = 0.4 - P(Type 3) = 0.25, P(Win|Type 3) = 0.5 ï¬What is probability of Win? {è´æ£‹}æ˜¯è¤‡é›œäº‹ä»¶ï¼Œå¿…é ˆåˆ†æˆä¸åŒé¡åˆ¥çš„å°æ‰‹åˆ†é …è¨è«–! ï¬Let W= Win, Fi = Type i : 1 1 2 2 3 3 ( ) ( | ) ( ) ( | ) ( ) ( | ) ( ) 0.5 0.3 0.25 0.4 0.25 0.5 0.375 P W P W F P F P W F P F P W F P F = + + = Ã— + Ã— + Ã— =
[Page 20]
Bayesâ€˜ Rule (è²å¼å®šç†) ï¬ Bayesâ€™ rule states the relation between P(Fi|G) and P(G|Fi ) ï¬ Plug the result of total probability for P(G) into the denominator below ï¬ è²æ°å®šç†åœ¨å·¥ç¨‹ä¸Šçš„æ‡‰ç”¨: å·¥ç¨‹ç³»çµ±è¨­è¨ˆ(å¦‚æ‰‹æ©Ÿé€šè¨Šæ¼”ç®—æ³•)ï¼Œæˆ–æ˜¯é€éäººå·¥æ™ºæ…§è™•ç†çš„é ä¼°èˆ‡åˆ†é¡å•é¡Œå¹¾ä¹éƒ½æ˜¯åœ¨å¾—çŸ¥æŸäº›äº‹ä»¶ (å¦‚å·²çŸ¥é‡æ¸¬è¨Šè™Ÿã€è’é›†åˆ°çš„è³‡æ–™)çš„å‰æä¸‹ä½œå‡ºæ±ºç­–åˆ¤æ–·ã€‚é€™æ‰€å·²çŸ¥çš„è¨Šè™Ÿã€è³‡æ–™å³æ˜¯è²æ°å®šç†ä¸­çš„çµ¦ å®šâ€œæ¢ä»¶â€(ä¸Šå¼çš„Gäº‹ä»¶) ã€‚ åœ¨å¾—çŸ¥Gäº‹ä»¶å¾Œï¼Œæˆ‘å€‘å°æ„Ÿèˆˆè¶£äº‹ä»¶çš„äº†è§£å¯èƒ½æœƒæœ‰æ‰€æ”¹è®Š(åŸæ©Ÿç‡vs.æ¢ä»¶æ©Ÿç‡)ã€‚ç³»çµ±å†æ ¹æ“šè¨ˆç®—å‡º çš„æ¢ä»¶æ©Ÿç‡å€¼åšå‡ºå°æ‡‰æ±ºç­–ã€‚å¦‚: æ‰‹æ©Ÿæ¥æ”¶åˆ°è¨Šè™Ÿï¼Œæ‰‹æ©Ÿæ™¶ç‰‡è©²åŸ·è¡Œä½•ç¨®é‹ç®—(åŸºé »æ¼”ç®—æ³•)ï¼Œç”¨ä»¥åµæ¸¬å‚³é€è¨Šè™Ÿæ˜¯0æˆ–1? ( ) ( | ) ( ) ( | ) ( ) ( ) ( | ) ( ) ( | ) ( ) i i i i i i j j j P F G P G F P F P F G P G P G P G F P F P G F P F âˆ© = = = âˆ‘
[Page 21]
Applications of Bayes' Rule Consider a more generic problem in statistical inference (çµ±è¨ˆæ¨ï¥) Given observed â€œeffectâ€ or â€œresultâ€ (event Gï¼Œçµæœ), infer the unobserved â€œcauseâ€ (one of the events F1, F2, â€¦, Fnï¼ŒåŸå› ). Procedure in Bayesian statistical inference ï¬ Assume we know the â€œpriorâ€ or â€œa prioriâ€ probabilities P(Fi ), for all i, and the conditional probabilities P(G|Fi ) ( P(Fi) å’ŒP(G|Fi ) è¼ƒå®¹æ˜“ä¼°ç®—ï¼Œä½†æˆ‘å€‘çš„ç›®çš„æ˜¯P(Fi|G) ) ï¬ Compute P(Fi|G), for all i (i.e., all possible causes F1â€¦Fn), i.e., the â€œposteriorâ€ probabilities (äº‹å¾Œæ©Ÿç‡) using Bayesâ€™ rule ï¬ Compare posterior probabilities for all i to infer which cause is the most likely to have led to G Choose the cause Fi that has the maximum posterior probability P(Fi|G) among P(F1|G) , P(F2|G), â€¦ P(Fn|G) ä¸Šè¿°åšæ³•ç¨±åšmaximum a posterior probability (MAP) åˆ¤æ–·æ³•å‰‡ã€‚ Why is it so special?
[Page 22]
An Example of Statistical Inference Observe a shade in a personâ€™s X-ray (this is event B, the â€œeffectâ€). We want to estimate the likelihood of three mutually exclusive and collectively exhaustive potential causes: 1. cause 1 (event A1) is that there is a malignant tumor 2. cause 2 (event A2) is that there is a nonmalignant tumor 3. cause 3 (event A3) corresponds to reasons other than a tumor What interests us are the posterior probabilities P(Ai|B) for i=1,2,3.
[Page 23]
Chess Example Again ï¬ Chess tournament, 3 types of opponents - P(Type 1) = 0.5, P(Win|Type 1) = 0.3 - P(Type 2) = 0.25, P(Win|Type 2) = 0.4 - P(Type 3) = 0.25, P(Win|Type 3) = 0.5 ï¬Given that I win, what is the probability I had a Type 1 opponent? ï¬Using Bayes' rule, 1 1 1 3 1 ( | ) ( ) ( | ) ( | ) ( ) 0.5 0.3 0.5 0.3 0.25 0.4 0.25 0.5 0.4 j j j P G F P F P F G P G F P F = = Ã— = Ã— + Ã— + Ã— = âˆ‘
[Page 24]
Radar Example ï¬G: Airpcraft is flying above ï¬F: Something registers on radar screen
[Page 25]
What is the probability that an aircraft is actually there given that the radar indicates a detection?
[Page 26]
ç¶“éèª¿æŸ¥ï¼ŒæŸ¯å—å·²ç¢ºä¿¡ç´é…’æœ‰60%çš„å¯èƒ½å·èµ°äº†åœ’å­å§å§çš„é‘½çŸ³ã€‚ç¾åœ¨ï¼Œæ–°çš„è­‰æ“šé¡¯ç¤ºçœŸæ­£çš„çŠ¯äºº æ˜¯å€‹å·¦æ’‡å­ã€‚ å·²çŸ¥åœ¨ç±³èŠ±å¸‚å·¦æ’‡å­äººæ•¸å è©²å¸‚ç¸½äººå£æ•¸æ¯”ä¾‹ç‚º20%ã€‚æŸ¯å—èªç‚ºï¼Œç´é…’å¦‚æœæ˜¯ä¸€å€‹å·¦æ’‡å­ï¼Œä»–å°±å¹¾ ä¹ç¢ºèªç´é…’æœƒæ˜¯çœŸæ­£çš„çŠ¯äººã€‚ è«‹å•ï¼ŒæŸ¯å—çš„ã€å¹¾ä¹ç¢ºèªã€è©²å¦‚ä½•ç”¨ç§‘å­¸æ–¹æ³•æ›ç®—å‡ºçœŸæ­£çš„æ©Ÿç‡å€¼? ä»¤äº‹ä»¶G={ç´é…’æ˜¯çœŸçŠ¯}ã€F={ç´é…’æ˜¯å·¦æ’‡å­} (Example 3f, A First Course in Probability, S. Ross, 8th edition) æŸ¯å—çš„æ¨ç†
[Page 27]
If we have a collection of three events, A1, A2, and A3, independence amounts to satisfying the four conditions 1. P(A1 âˆ©A2) = P(A1)P(A2) 2. P(A1 âˆ©A3) = P(A1)P(A3) 3. P(A2 âˆ©A3) = P(A2)P(A3) 4. P(A1 âˆ©A2 âˆ©A3) = P(A1)P(A2)P(A3) ï¬ The first three conditions are known as pairwise independence ï¬ The 4 conditions must be satisfied simultaneously for us to claim indep. of , A1, A2 and A3 Pairwise independence does NOT imply the 4th condition Conversely, the 4th condition does NOT imply pairwise independence (See examples 1.22 and 1.23) ä¸‰å€‹äº‹ä»¶çš„ç¨ç«‹æ€§è³ª