[
  {
    "source": "topic5.txt",
    "category": "Probability",
    "chunk_index": 0,
    "page": 1,
    "text": "1 Topic 5: Multiple Discrete Random Variables Lecture Outline ï¬ Joint PMF and Marginal PMF ï¬ Linearity of Expectation ï¬ Conditional PMF Total Probability Theorem (TPT) in Discrete Random Variables (ç¬¬ä¸€å€‹TPTè®Šå½¢) ï¬ Independent Random Variables ï¬ Conditional Expectation Reading : Textbook 2.5- 2.7"
  },
  {
    "source": "topic5.txt",
    "category": "Probability",
    "chunk_index": 1,
    "page": 1,
    "text": "2 Multiple Random Variables Quite naturally, we have to deal with multiple RVs in real-world problems: The money we earn in several consecutive plays of poker games The numbers of 3 pointers Steph Curry can make in 4 different quarters In a communication system, the receiver signal can often be modeled as the signal that contains the (unknown) transmitted signal plus (unknown) noise We are particularly interested in knowing: How different random variables are related to each other?"
  },
  {
    "source": "topic5.txt",
    "category": "Probability",
    "chunk_index": 2,
    "page": 1,
    "text": "random variables are related to each other? ä¸åŒéš¨æ©Ÿè®Šæ•¸é–“ä¹‹é—œè¯æ€§(æ­£ç›¸é—œã€è² ç›¸é—œã€é›¶ç›¸é—œã€ç¨ç«‹ã€æ¢ä»¶æ©Ÿç‡ã€æ¢ä»¶æœŸæœ›å€¼?) How to learn the behavior of the sum of all RVs? How to learn the behavior or the true value of one RV when it is buried in the sum with other random variables? Ex: å¦‚ä½•å¾æ¥æ”¶è¨Šè™Ÿ(Y)=å‚³é€è¨Šè™Ÿ(S)+é›œè¨Š(N) ä¸­æ“·å–S çš„è³‡è¨Š"
  },
  {
    "source": "topic5.txt",
    "category": "Probability",
    "chunk_index": 3,
    "page": 3,
    "text": "3 Joint PMF We can fully describe two discrete RVs X and Y by the joint PMF. Suppose two discrete RVs X and Y are defined over the same sample space, the joint PMF pX,Y(x,y) is defined by pX,Y(x,y) = P(X=x and Y=y) = P( {Ï‰: X(Ï‰) = x and Y(Ï‰) = y} ) From the additivity and normalization axioms of probability, we must have Î£x,y pX,Y(x,y) = 1 Example: Rolling two 4-sided die. Let X be the outcome of the first roll and Y of the second, then pX,Y(x,y) = 1/16; for x=1,2,3,4; y=1,2,3,4"
  },
  {
    "source": "topic5.txt",
    "category": "Probability",
    "chunk_index": 4,
    "page": 4,
    "text": "4 Joint PMF and Marginal PMF Suppose X and Y are two random variables defined on a common experiment with a joint PMF pX,Y(x,y) . The marginal PMFs pX(x) and pY(y) can be obtained from joint PMF as follows Remarks: ï¬ The above relation can be explained by the total probability theorem ï¬ The marginal PMF is just the â€œindividualâ€ PMF pX(x) or pY(y) (å–®ç¨RVçš„PMFã€åŸæœ¬å€‹åˆ¥çš„PMFã€é‚Šéš›PMF)\n\n5 Example: Calculating Marginal PMF from Joint PMF Example 2.9:"
  },
  {
    "source": "topic5.txt",
    "category": "Probability",
    "chunk_index": 5,
    "page": 6,
    "text": "6 Functions of Multiple Random Variables A function Z = g(X, Y ) of the random variables X and Y defines another random variable. Its PMF can be calculated from the joint PMF pX,Y according to The expected value rule for functions takes the following form Example: We are often interested in finding E[XY], which measures the similarity between X and Y."
  },
  {
    "source": "topic5.txt",
    "category": "Probability",
    "chunk_index": 6,
    "page": 7,
    "text": "7 Linearity of Expectation As an example, consider the linear function g(X,Y)=aX+bY+c of random variables X and Y, where a, b, and c are scalars (constants). The expectation of g(X,Y) is given by Expectation is linear We see that the expectation of the sum of random variables is the sum of the expectations. Do Example 2.9 and Example 2.11."
  },
  {
    "source": "topic5.txt",
    "category": "Probability",
    "chunk_index": 7,
    "page": 8,
    "text": "8 More Than Two Random Variables Consider 3 random variables X, Y, and Z. Joint PMF For all possible triplets of numerical values (x,y,z) Marginal PMF from Joint PMF Linearity of Expectation This is an extremely useful relation!"
  },
  {
    "source": "topic5.txt",
    "category": "Probability",
    "chunk_index": 8,
    "page": 9,
    "text": "9 Example - Mean of Binomial R.V. One way to define a Binomial random variable is the following: Flip a coin (with bias p) n times independently and let Xi = 1 if the i-th toss is a head and 0 if it is a tail. Define the random variable ğ‘Œğ‘Œ= âˆ‘ğ‘–ğ‘–=1 ğ‘›ğ‘› ğ‘‹ğ‘‹ğ‘–ğ‘–. Then, ğ‘Œğ‘Œis exactly a binomial RV."
  },
  {
    "source": "topic5.txt",
    "category": "Probability",
    "chunk_index": 9,
    "page": 10,
    "text": "10 Conditional PMF The conditional PMF of a random variable X, conditioned on a particular event A with P(A) > 0, is defined by In the special case A={Y=y}, we have a PMF conditioned on a random variable Y\n\nVisualization of Conditional PMF 11"
  },
  {
    "source": "topic5.txt",
    "category": "Probability",
    "chunk_index": 10,
    "page": 12,
    "text": "Visualization of Conditional PMF 11\n\n12 Properties of Conditional PMF ï¬ Conditional PMF is just conditional probability in different notations Satisfies the 3 probability axioms ï¬ Normalization ï¬ Chain rule ï¬ Total probability Theorem This is the first variant (è®Šå½¢) of the total probability theorem. We will see three more later in Chapter 3. Please do Example 2.14 and Example 2.15."
  },
  {
    "source": "topic5.txt",
    "category": "Probability",
    "chunk_index": 11,
    "page": 13,
    "text": "13 Example Example (Problem 43 on page 134) Suppose that X and Y are independent and identically distributed (i.i.d.) geometric random variables with parameter p, then Remarks:  Physical interpretations (ç‰©ç†æ„ç¾©)?  The acronym â€œi.i.d.â€ appears very often. So please memorize what that means. .1 , ,1 , 1 1 ) | ( âˆ’ = âˆ’ = = + = n i n n Y X i X P ï‹"
  },
  {
    "source": "topic5.txt",
    "category": "Probability",
    "chunk_index": 12,
    "page": 14,
    "text": "14 Independence Definition: We say that two random variables X and Y are independent if ï¬ Independence means the experimental value of Y tells us nothing about the value of X ï¬ Equivalent to saying that the events { Ï‰: X(Ï‰) = x} and {Ï‰ : Y (Ï‰) = y} are independent events for all possible outcomes x and y ï¬ The above definition of independence is equivalent to"
  },
  {
    "source": "topic5.txt",
    "category": "Probability",
    "chunk_index": 13,
    "page": 14,
    "text": "15 Important Facts from Independent RVs ï¬ If X and Y are independent random variables, then we can prove that ç‚ºä»€éº¼æˆ‘å€‘æœƒå°E[XY]æ˜¯å¦ç­‰æ–¼E[X] E[Y] æ„Ÿèˆˆè¶£? If X and Y are independent, then we have E[XY]= E[X] E[Y]. On the contrary, if E[XY]= E[X] E[Y], can we say that X and Y are independent? The answer is NO. We will look into the above two issues more deeply in Chapter 4. ï¬ If X and Y are independent random variables, then g(X) and h(Y ), for any functions g and h, are also independent. (See the proof in"
  },
  {
    "source": "topic5.txt",
    "category": "Probability",
    "chunk_index": 14,
    "page": 14,
    "text": "g and h, are also independent. (See the proof in Problem 44.) If X and Y are independent random variables, then for any functions g and h"
  },
  {
    "source": "topic5.txt",
    "category": "Probability",
    "chunk_index": 15,
    "page": 16,
    "text": "16 Important Facts from Independent RVs ï¬ If X and Y are independent random variables, then But in general, this relation is not always true. That is, if X and Y are NOT independent, then we do NOT know whether the above relation is true. ï¬ å¯ä»¥å»¶ä¼¸åˆ°: è‹¥X1,X2,â€¦,Xn ç‚ºç¨ç«‹éš¨æ©Ÿè®Šæ•¸(independent RVs), å‰‡"
  },
  {
    "source": "topic5.txt",
    "category": "Probability",
    "chunk_index": 16,
    "page": 17,
    "text": "17 Multiple Independent Random Variables A collection of random variables { X1,X2, . . . ,Xn} is said to be independent if the n-fold joint PMF is a product of marginal PMFs: In the special case where the marginal probabilities ğ‘ğ‘ğ‘‹ğ‘‹ğ‘–ğ‘–(ğ‘¥ğ‘¥ğ‘–ğ‘–) are all the same, we say that these n random variables are independent and identically distributed (i.i.d.)"
  },
  {
    "source": "topic5.txt",
    "category": "Probability",
    "chunk_index": 17,
    "page": 18,
    "text": "18 Example - Variance of Binomial One way to define a Binomial random variable is the following: Flip a coin (with bias p) n times independently and let Xi = 1 if the i-th toss is a head and 0 if it is a tail. Define the random variable ğ‘Œğ‘Œ= âˆ‘ğ‘–ğ‘–=1 ğ‘›ğ‘› ğ‘‹ğ‘‹ğ‘–ğ‘–. Then, Y is exactly a binomial RV."
  },
  {
    "source": "topic5.txt",
    "category": "Probability",
    "chunk_index": 18,
    "page": 19,
    "text": "19 Example â€“ Sample Mean Let X1,X2,â€¦,Xn be n i.i.d. random variables with mean Âµ and variance Ïƒ2. The sample mean is defined by We often use sample mean to estimate the mean value of Xi What are the mean and variance of Sn? As n grows, the variance of the sample mean decreases"
  },
  {
    "source": "topic5.txt",
    "category": "Probability",
    "chunk_index": 19,
    "page": 19,
    "text": "20 Conditional Expectation Definitions Conditional expectation of X given an event A: Conditional expectation of X given Y=y : ï¬ Conditional expectation E[X|Y=y] is a function of y, itâ€™s a numerical value depending on y. On the other hand, E[X|Y] is a function of the random variable Y, and is therefore a random variable. è«‹èªªæœè‡ªå·±ï¼ŒE[X|Y] æ˜¯éš¨æ©Ÿè®Šæ•¸ï¼Œè€ŒE[X|Y=y] æ˜¯ä¸€å€‹å’Œy æœ‰é—œçš„å¯¦æ•¸(ä¹Ÿå°±æ˜¯ä»¥y ç‚ºè®Šæ•¸çš„å‡½æ•¸å€¼) ï¬ Conditional probability (in particular the posterior probability) and conditional expectation play very critical roles"
  },
  {
    "source": "topic5.txt",
    "category": "Probability",
    "chunk_index": 20,
    "page": 19,
    "text": "conditional expectation play very critical roles in system designs under uncertainty."
  },
  {
    "source": "topic5.txt",
    "category": "Probability",
    "chunk_index": 21,
    "page": 21,
    "text": "21 Example If X and Y are i.i.d. geometirc random variables with parameters p. Find the conditional expected value of X, given that X+Y=n."
  },
  {
    "source": "topic5.txt",
    "category": "Probability",
    "chunk_index": 22,
    "page": 22,
    "text": "22 Total Expectation Theorem** Total expectation theorem states unconditional expectation can be calculated using conditional expectation, just like the counterpart of unconditional probability. Please do Example 2.16 and Example 2.17. Remark: Comparing with the expected value rule, we can re-write the total expectation theorem more compactly as é€™æ˜¯ç¶“å¸¸ä½¿ç”¨åˆ°çš„é—œä¿‚å¼!"
  },
  {
    "source": "topic5.txt",
    "category": "Probability",
    "chunk_index": 23,
    "page": 23,
    "text": "23 Example â€“ Sum of a Random Number of Random Variables Consider the sum where N is a random variable that takes positive integer values, and X1,X2,â€¦,XN are i.i.d. random variables. We assume that N is independent with Xi for all i. Then, we have where E[Xi]=m for all i. Practical applications: N : the number of customers entering a department store Xi: amount of money spent by the ith customer Y : total money spent by customers in the store E[Y]: expected money spent in the store"
  },
  {
    "source": "topic5.txt",
    "category": "Probability",
    "chunk_index": 24,
    "page": 24,
    "text": "24 Properties of Conditional Expectation ï¬ Conditional expectation is simply expectation with respect to a conditional PMF. It inherits many properties from ordinary expectation  Linearity  Expected value of functions of random variables ï¬E[Y|Y=y]=? ï¬ When conditioning on a random variable, that random variable plays the role of a constant"
  },
  {
    "source": "topic5.txt",
    "category": "Probability",
    "chunk_index": 25,
    "page": 25,
    "text": "25 Conditional Probability Conditioned on Random Variable è§€å¿µæ¾„æ¸… For independent random variables X and Y, which of the following statements is correct, for an appropriate function g() and set A? Example: Toss a dice twice, and let the outcome for the 1st toss and the 2nd toss be X1 and X2, respectively. What is the probability ğ‘ƒğ‘ƒ ğ‘‹ğ‘‹1 + ğ‘‹ğ‘‹2 â‰¤8 âˆ©ğ‘‹ğ‘‹1 = 5 =? What is the probability ğ‘ƒğ‘ƒğ‘‹ğ‘‹1 + ğ‘‹ğ‘‹2 â‰¤8 | ğ‘‹ğ‘‹1 = 5 =?"
  }
]