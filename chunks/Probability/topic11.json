[
  {
    "source": "topic11.txt",
    "category": "Probability",
    "chunk_index": 0,
    "page": 1,
    "text": "Topic 11: Limit Theorems Outline ï¬Markov Inequality ï¬Chebyshev Inequality ï¬Weak Law of Large Number (LLN) å¤§ï¥©æ³•å‰‡ ï¬The Central Limit Theorem (CLT) ä¸­å¤®æ¥µé™å®šï§¤ ï¬Convergence of a Sequence of RVs éš¨æ©Ÿåºï¦œçš„æ”¶æ–‚æ€§ Reading: Textbook Section 5.1~5.5 1"
  },
  {
    "source": "topic11.txt",
    "category": "Probability",
    "chunk_index": 1,
    "page": 2,
    "text": "2 ï¬Why study limit theorems? Study the asymptotic behavior of sequences of RVs. Weâ€™d like to know whether a sequence of RVs converges (å¦‚éµç©¹é˜²ç¦¦ç³»çµ±è¦èƒ½é–å®šä¸¦æŒçºŒè¿½è¹¤ç›´åˆ°æ“Šä¸­ç›®æ¨™) å¤§ï¥©æ³•å‰‡(Law of Large Numbers) é™³è¿°sample mean à¬µ à¯¡âˆ‘ ğ‘‹à¯œ à¯¡ à¯œà­€à¬µ åœ¨æ¨£æœ¬ ï¥©å¾ˆå¤§æ™‚æœƒæ”¶æ–‚åˆ°çœŸå¯¦çš„æœŸæœ›å€¼ã€‚ æ‡‰ç”¨: å¯ä¼°è¨ˆç¾¤é«”ä¸­æŸç¾è±¡å­˜åœ¨å€‹ï¥©(æŸäº‹ä»¶ç™¼ç”Ÿæ¬¡ï¥©)ã€‚åœ¨ç¾¤é«”æ¨£æœ¬éå¸¸ å¤§æ™‚ï¼Œæ­¤ä¼°è¨ˆæ„ˆæº–ç¢ºã€‚ æˆ‘å€‘ç¶“å¸¸å°ä¸€ï¦šï¤…i.i.d.éš¨æ©Ÿè®Šï¥©ä¹‹ç¸½å’Œæ„Ÿèˆˆè¶£ï¼Œå¦‚è¨ˆç®—å€‹ï¥©ã€å¤šæ¬¡è³½ å±€è¼¸è´ç¸½åˆã€é•·éšŠä¼ç­‰å¾…æ™‚é–“ç­‰ã€‚ä¸­å¤®æ¥µé™å®šï§¤(Central Limit Theorem)å®£å‘Šæˆ‘å€‘å¯å€Ÿç”¨å¸¸æ…‹åˆ†ä½ˆï¤­è¿‘ä¼¼æ­¤ç¸½å’Œçš„PDF Introduction"
  },
  {
    "source": "topic11.txt",
    "category": "Probability",
    "chunk_index": 2,
    "page": 3,
    "text": "3 ï¬Law of large numbers and Central Limit Theorem are related to the sample mean. Letâ€™s have a quick review. ï¬X1,X2, . . .Xn are i.i.d. RVs with E[Xi]=Î¼, var(Xi)=Ïƒ2 Sample mean: Mn = (X1+X2+. . .+Xn ) /n Then, Review of Sample Mean 2 [ ] ; var( ) n n E M M n ï³ ï­ ï€½ ï€½"
  },
  {
    "source": "topic11.txt",
    "category": "Probability",
    "chunk_index": 3,
    "page": 4,
    "text": "Markov Inequality If a random variable X can only take nonnegative values, then we must have the following Markov inequality Remark: åƒ…éœ€çŸ¥é“æœŸæœ›å€¼å³å¯! Markov inequality provides a loose bound, but is easy to calculate! Example 5.1: Uniform in [0,4] (p.266) 4 2 3 Markov: Pr( 3) 0.67; Exact: Pr( 3) 0.25 X X ï‚³ ï‚£ ï€½ ï‚³ ï€½"
  },
  {
    "source": "topic11.txt",
    "category": "Probability",
    "chunk_index": 4,
    "page": 5,
    "text": "Chebyshev Inequality If X is a random variable with mean Î¼ and variance Ïƒ2, then Remarks:  Markov and Chebyshev inequalities allow us to derive bounds on probabilities when only the mean, or both the mean and the variance are available  In general, Markov and Chebyshev inequality are NOT very tight upper bounds, as the required information is only the mean and the variance. (Examples 5.1 and 5.2) 5"
  },
  {
    "source": "topic11.txt",
    "category": "Probability",
    "chunk_index": 5,
    "page": 6,
    "text": "Chebyshev Inequality In general, Markov and Chebyshev inequality are NOT very tight upper bound. Example 5.2: Let ğ‘‹be uniform in [0,4]. Use the Chebyshev inequality to bound the probability of ğ‘‹àµ†2 àµ’1ï€® 6"
  },
  {
    "source": "topic11.txt",
    "category": "Probability",
    "chunk_index": 6,
    "page": 6,
    "text": "Chebyshev Inequality Example 1: Weak Law of Large Numbers (LLN) Let X1, X2, . . ., Xn be a sequence of independent and identically distributed (i.i.d.) RVs with mean Î¼ and finite variance Ïƒ2. For every ğœ–> 0 (no matter how small it is), we have the following (weak) law of large numbers where Mn =(X1+X2+â€¦+Xn )/n is the sample mean. The weak LLN (å¤§ï¥©æ³•å‰‡) says that the sample mean will be very close to the true mean, when the sample size is sufficiently large. (Proof) Use Chebyshev inequality! 7"
  },
  {
    "source": "topic11.txt",
    "category": "Probability",
    "chunk_index": 7,
    "page": 8,
    "text": "Chebyshev Inequality Example: The Pollsterâ€™s Problem The Pollsterâ€™s Problem (æ°‘æ„èª¿æŸ¥å•é¡Œ) You may often see something like this, particularly during elections: ã€Œé€™æ¬¡é¸å‰æ°‘æ„èª¿æŸ¥ï¼ŒXXçš„æ”¯æŒï¥¡ç´„ç‚º52%ã€‚æ­¤çµæœæœ‰ 95% ä¿¡å¿ƒæ°´æº–ï¼ŒæŠ½æ¨£èª¤å·®æ–¼æ­£è² 3% ä»¥å…§ã€‚ã€ Questions: 1) ç›´è¦ºä¸Šï¼ŒæŠ½æ¨£æ¨£æœ¬ï¥©è¶Šå¤šï¼Œå‰‡ä¼°è¨ˆå‡ºï¤­çš„æ”¯æŒï¥¡è¶Šæ¥è¿‘çœŸå¯¦ã€‚ä½†ï¼Œ æœ€å°‘éœ€æŠ½æ¨£å¹¾ä»½ï¼Œæ‰èƒ½ä¿è­‰æœ‰ä¸Šè¿°ä¹‹æŠ½æ¨£èª¤å·®èˆ‡ä¿¡å¿ƒæ°´æº–? 2) å¦å¤–ä¸€å€‹å•æ³•æ˜¯ï¼Œæ°‘èª¿å…¬å¸é ç®—åƒ…èƒ½å…è¨±æŠ½æ¨£1000ä»½ã€‚ï¥´æ¬²ä¿è­‰é  ä¼°çš„æ”¯æŒï¥¡å¯æœ‰95%ä¿¡å¿ƒæ°´æº–ï¼Œé‚£æŠ½æ¨£èª¤å·®è¦å¦‚ä½•ä¿®æ­£? 8"
  },
  {
    "source": "topic11.txt",
    "category": "Probability",
    "chunk_index": 8,
    "page": 9,
    "text": "The Pollsterâ€™s Problem Example 5.5: Accuracy of Polling (p.270) (æ°‘èª¿ï¥©æ“šçš„æº–ç¢ºï¥¡) ï¬The ith personâ€™ polled: ï¬Fraction of â€œYesâ€ : ïµp: (unknown) fraction of population voting â€œYesâ€: ïµThe question here is: 95% ä¿¡å¿ƒæ°´æº–ï¼ŒæŠ½æ¨£èª¤å·®æ–¼æ­£è² 1% ä»¥å…§ï¼Œå‰‡éœ€è¦æŠ½æ¨£å¹¾ä»½? 9 1 If \"Yes\" 0 If \"No\" i X ïƒ¬ ï€½ïƒ­ ïƒ® 1 2 n n X X X M n ï€« ï€« ï€« ï€½ ïŒ x p ï­ï€½"
  },
  {
    "source": "topic11.txt",
    "category": "Probability",
    "chunk_index": 9,
    "page": 10,
    "text": "The Pollsterâ€™s Problem ï¬Accuracy of Polling (p.270) (æ°‘èª¿ï¥©æ“šçš„æº–ç¢ºï¥¡) ïµä½¿ç”¨Chebyshev: In fact, Xi is i.i.d. Bernoulli RV: Remarks: 1. Chebyshev ç®—å‡ºçš„n å¯ä¿è­‰ï¥§ç­‰å¼æˆï§·ï¼Œbutâ€¦ 2. CLT can also do the tricks, and give a much smaller n (i.e., fewer samples. See page 14.) 10 Pr( 0.01) 0.05 n X M ï­ ï€­ ï‚³ ï‚£ 2 2 Pr( ) x n x M n ï³ ï­ ï¥ ï¥ ï€­ ï‚³ ï‚£ 2 14 ; (1 ) x x p p p ï­ ï³ ï€½ ï€½ ï€­ ï‚£ 1 Pr( 0.01) 50,000 0.0004 n M p n n ï€­ ï‚³ ï‚£ ïƒ ï€¾ x p ï­ï€½ 95% ä¿¡å¿ƒæ°´æº–ï¼ŒæŠ½æ¨£èª¤å·®æ–¼æ­£è² 1% ä»¥å…§"
  },
  {
    "source": "topic11.txt",
    "category": "Probability",
    "chunk_index": 10,
    "page": 11,
    "text": "The Central Limit Theorem (CLT) Central Limit Theorem (ä¸­å¤®æ¥µé™å®šï§¤) Let X1,X2, . . ., Xn be a sequence of i.i.d. RVs with mean Î¼ and variance Ïƒ2, and define Then, the CDF of Zn converges to the standard normal CDF ï†(z) in the sense that Remarks: Note that X1,X2, â€¦,Xn can be of any distributions (this is why CLT is so powerful), and the CLT is also true. ï¥§ï¥X1,X2, â€¦,Xn åŸæœ¬çš„çµ±è¨ˆï¨ˆç‚ºå¦‚ä½•ï¼ŒZn åœ¨nå¾ˆå¤§æ™‚æœƒè¶¨è¿‘æ–¼å¸¸æ…‹åˆ†ä½ˆ 11"
  },
  {
    "source": "topic11.txt",
    "category": "Probability",
    "chunk_index": 11,
    "page": 11,
    "text": "The Central Limit Theorem (CLT) Central Limit Theorem (ä¸­å¤®æ¥µé™å®šï§¤) Let X1,X2, . . ., Xn be a sequence of i.i.d. RVs with mean Î¼ and variance Ïƒ2, and define Then, the CDF of Zn converges to the standard normal è©²å¦‚ä½•å»ï§¤è§£ä¸Šå¼Znçš„çµæ§‹? æ¸›å»n Î¼ çš„æ„ç¾©ï¼Œåˆ†æ¯çš„ç‰©ï§¤æ„ç¾©å„ç‚ºä½•? CLT well models many practical real-world problems. Sum of i.i.d. RVs with arbitrary distributions having finite mean and variance can be approximated by Gaussian ïƒ¼The number of occurrence of a certain event, the sample mean ïƒ¼Noise, channel effects in the"
  },
  {
    "source": "topic11.txt",
    "category": "Probability",
    "chunk_index": 12,
    "page": 11,
    "text": "the sample mean ïƒ¼Noise, channel effects in the received signal of your smartphone 12"
  },
  {
    "source": "topic11.txt",
    "category": "Probability",
    "chunk_index": 13,
    "page": 13,
    "text": "Proof of CLT (p.290) ï¬Sum of indep. RVs ïƒ use moment generating function 13\n\n14 ï¬To make a â€œgoodâ€ Gaussian, how large is n? â€œNâ€ in CLT n=2 n=32 n=4 n=1\n\n15 ï¬The shape of PDF of X has â€œsomeâ€ impact on n. â€œNâ€ in CLT (2) n=8 n=1 n=32 n=18"
  },
  {
    "source": "topic11.txt",
    "category": "Probability",
    "chunk_index": 14,
    "page": 13,
    "text": "The Pollsterâ€™s Problem (p.277) ï¬For the ith person: ï¬Fraction of â€œYesâ€: ïµp: fraction of population voting â€œYesâ€ =Î¼X ïµSuppose we want: ïµCLT facilitates calculations of the probability: 16 1 If \"Yes\" 0 If \"No\" i X ïƒ¬ ï€½ïƒ­ ïƒ® 1 2 n n X X X M n ï€« ï€« ï€« ï€½ ïŒ 2 14 (1 ) x p p ï³ï€½ ï€­ ï‚£ 2 0.01 1.96 9604 n n ïƒ— ïƒ— ï‚³ ïƒ ï€¾ Pr( 0.01) Pr(| | 0.02 ) 0.05 n X M Z n ï­ ï€­ ï‚³ ï‚» ï‚³ ï‚£ Pr( 0.01) 0.05 n X M ï­ ï€­ ï‚³ ï‚£ 1 2 n n X X X M n ï€« ï€« ï€« ï€½ ïŒ 1 2 1 2 0.01 0.01 n n X X X np X X X np n n n ï³ ï³ ï€« ï€« ï€« ï€­ ï€« ï€« ï€« ï€­ ï‚³ ï‚® ï‚³ ïŒ ïŒ 95%"
  },
  {
    "source": "topic11.txt",
    "category": "Probability",
    "chunk_index": 15,
    "page": 13,
    "text": "X X X np n n n ï³ ï³ ï€« ï€« ï€« ï€­ ï€« ï€« ï€« ï€­ ï‚³ ï‚® ï‚³ ïŒ ïŒ 95% ä¿¡å¿ƒæ°´æº–ï¼ŒæŠ½æ¨£èª¤å·®æ–¼æ­£è² 1% ä»¥å…§"
  },
  {
    "source": "topic11.txt",
    "category": "Probability",
    "chunk_index": 16,
    "page": 17,
    "text": "17 CLT Applied to Binomial ï¬Bernoulli: ï¬Binomial: ï¬Approximation: Example: n=36, p=0.5; find Pr(Sn=19) Exact answer: 2 ; (1 ) x x p p p ï­ ï³ ï€½ ï€½ ï€­ 1 2 n n S X X X ï€½ ï€« ï€« ï€« ïŒ 2 ; (1 ) S S np np p ï­ ï³ ï€½ ï€½ ï€­ (1 ) S np np p ï€­ ï‚® ï€­ CDF of standard normal 36 36 1 0.1251 19 2 ïƒ¦ ïƒ¶ïƒ¦ ïƒ¶ ï€½ ïƒ§ ïƒ· ïƒ§ ïƒ·ïƒ¨ ïƒ¸ ïƒ¨ ïƒ¸"
  },
  {
    "source": "topic11.txt",
    "category": "Probability",
    "chunk_index": 17,
    "page": 18,
    "text": "18 De Moivre-Laplace Approximation to Binomial ï¬The Â½ Correction for Binomial Approximation Let Sn be normalized to zero mean and unit variance: Pr(kâ‰¤ Snâ‰¤l) â‰ˆ Î¦(l+0.5)âˆ’Î¦(k âˆ’ 0.5) Ex: n=36, p=0.5; Pr(Sn=19) = Pr(18.5â‰¤Snâ‰¤19.5)"
  },
  {
    "source": "topic11.txt",
    "category": "Probability",
    "chunk_index": 18,
    "page": 19,
    "text": "19 ï¬Deterministic limit: a sequence ïƒ a number an converges to a: an eventually gets and stays (arbitrarily) close to a Def: For every Îµ>0 there exists n0, such that for all nâ‰¥n0, |an âˆ’a |â‰¤Îµ. ï¬Stochastic limits: there exists several definitions Convergence â€œin probabilityâ€ Convergence â€œalmost surelyâ€ or â€œwith probability 1â€ Convergence lim n n a a ï‚®ï‚¥ ï€½"
  },
  {
    "source": "topic11.txt",
    "category": "Probability",
    "chunk_index": 19,
    "page": 20,
    "text": "20 ï¬Y1,Y2, . . . is a sequence of RVs, and let a be a real number. We say Yn converges to a in probability, if for every Îµ>0, we have ï¬(Almost) all of the PMF/PDF of Yn eventually gets concentrated (arbitrarily) close to a ï¬ä¾‹å­: (å¤§æ•¸æ³•å‰‡) The weak law of large numbers ïƒ the sample mean converges in prob to the true mean. ï¬Accuracy level: Îµ, Confidence level: Î´; Convergence â€œin Probabilityâ€ lim Pr( ) 0 n n Y a ï¥ ï‚®ï‚¥ ï€­ ï‚³ ï€½ 0 Pr( ) , for all n Y a n n ï¥ ï¤ ï€­ ï‚³ ï‚£ ï‚³"
  },
  {
    "source": "topic11.txt",
    "category": "Probability",
    "chunk_index": 20,
    "page": 21,
    "text": "21 Example 5.6: Consider a sequence of independent random variables ğ‘‹à¯¡ that are uniform in [0,1]. Let ğ‘Œà¯¡àµŒmináˆ¼ğ‘‹à¬µ, â€¦ , ğ‘‹à¯¡áˆ½ Does ğ‘Œà¯¡converge to in probability? Example (p.272)\n\n22 ï¬Consider a sequence of RVs Yn with the following PMFs. ï¬Does Yn converge? Yes. ï¬What is E[Yn]? â€œnâ€ Example (p.272) 1 lim Pr( 0 ) lim 0 n n n Y n ï¥ ï‚®ï‚¥ ï‚®ï‚¥ ï€­ ï‚³ ï€½ ï€½ 2 n 2 1 1 0 1 n n n n ïƒ¦ ïƒ¶ ïƒ— ï€­ ï€« ï€½ ïƒ§ ïƒ· ïƒ¨ ïƒ¸"
  },
  {
    "source": "topic11.txt",
    "category": "Probability",
    "chunk_index": 21,
    "page": 23,
    "text": "23 ï¬Y1,Y2, . . . is a sequence of RVs, and let c be a real number. We say Yn converges to c with probability 1, if ï¬Remarks:  lim à¯¡â†’à®¶ğ‘Œà¯¡àµŒğ‘ å¯«å¾—ï¨ç¢ºä¸€äº›æ‡‰ç‚ºğœ”: lim à¯¡â†’à®¶ğ‘Œà¯¡áˆºğœ”áˆ»àµŒğ‘ å¤§æ‹¬è™Ÿè¡¨ç¤ºä¸€å€‹äº‹ä»¶ï¼Œæ­¤äº‹ä»¶ç™¼ç”Ÿæ©Ÿï¥¡ç‚º1 æ™‚ç¨±Yn converges to c with probability 1 ï¬ éš¨æ©Ÿè®Šï¥©æ‰€æ§‹æˆçš„åºï¦œï¼Œå…¶æ”¶æ–‚çš„ï¥§åŒå®šç¾©æœƒåœ¨ç ”ç©¶æ‰€ â€œéš¨æ©Ÿéç¨‹â€èª²ç¨‹ä¸­æœ‰ï¤è©³ç´°è§£ï¥¯ Convergence â€œwith Probability 1â€ Pr(lim ) 1 n n Y c ï‚®ï‚¥ ï€½ ï€½"
  },
  {
    "source": "topic11.txt",
    "category": "Probability",
    "chunk_index": 22,
    "page": 24,
    "text": "24 Example Consider a sample space Î© àµŒáˆ¾0,1áˆ¿, the closed interval between 0 and 1. Assume that each sample point ğœ–âˆˆÎ© follows continuous uniform distribution. Then, the random sequence ğ‘‹à¯¡ğœ–àµŒexpáˆºàµ†ğ‘›à¬¶áˆºğœ–àµ†1/ğ‘›áˆ»áˆ»converges to 0 with probability 1. Example -- Convergence with Probability 1\n\n25 Example 5.14 Consider a sequence of independent random variables ğ‘‹à¯¡ that are uniform in [0,1]. Let ğ‘Œà¯¡àµŒmináˆ¼ğ‘‹à¬µ, â€¦ , ğ‘‹à¯¡áˆ½ Show that ğ‘Œà¯¡converge to 0 with probability 1. Example"
  },
  {
    "source": "topic11.txt",
    "category": "Probability",
    "chunk_index": 23,
    "page": 24,
    "text": "26 ï¬Y1,Y2, . . . is a sequence of RVs, and let c be a real number. We say Yn converges to c with probability 1, if ï¬Remarks Convergence with probability 1 å®šç¾©ä¸­æœ‰å‚³çµ±ï¥©ï¦œæ”¶æ–‚çš„æ¦‚ï¦£(see p.19)ã€‚ æ¨£æœ¬ç©ºé–“ä¸­ï¥´å¹¾ä¹æ¯ä¸€å€‹sample point å°æ‡‰åˆ°çš„ï¥©ï¦œï¨¦æœƒæ”¶æ–‚ï¼Œåƒ…éƒ¨åˆ†çš„ sample pointså°æ‡‰åˆ°çš„ï¥©ï¦œï¥§æ”¶æ–‚ï¼Œï¥´é‚£äº›å°æ‡‰åˆ°ï¥§æ”¶æ–‚ï¥©ï¦œçš„sample pointsæ‰€æˆé›†åˆç™¼ç”Ÿæ©Ÿï¥¡æ˜¯ï¦²ï¼Œå³ç‚ºæ­¤è™•æ‰€è¬‚æ”¶æ–‚with probability 1 æˆ–ç¨±ç‚º convergence almost surely Convergence with probability 1 implies convergence in probability. The converse is not necessarily true. (Problem 15*, p.292) Example 5.15 (p.282) says a random"
  },
  {
    "source": "topic11.txt",
    "category": "Probability",
    "chunk_index": 24,
    "page": 24,
    "text": "15*, p.292) Example 5.15 (p.282) says a random sequence may converge in probability, but not with probability 1 Convergence â€œwith Probability 1â€ Pr(lim ) 1 n n Y c ï‚®ï‚¥ ï€½ ï€½"
  },
  {
    "source": "topic11.txt",
    "category": "Probability",
    "chunk_index": 25,
    "page": 27,
    "text": "27 Example 5.15 Consider a discrete time arrival process. The set of times is ğˆà¯àµŒ2à¯, 2à¯àµ…1, â€¦ , 2à¯à¬¾à¬µàµ†1 for k=1,2,â€¦. During each ğˆà¯, there is exactly one arrival, and all times within an interval are equally likely. The arrival times in different intervals are assumed to be independent. Define ğ‘Œà¯¡=1 if there is an arrival at time n, and ğ‘Œà¯¡=0 if there is no arrival. Show that ğ‘Œà¯¡converge to 0 in probability, but NOT with probability 1 Example"
  },
  {
    "source": "topic11.txt",
    "category": "Probability",
    "chunk_index": 26,
    "page": 27,
    "text": "The Strong Law of Large Numbers ï¬Let X1, X2, . . . be a sequence of i.i.d. RVs with mean Î¼. Let , ï¬Sample space: set A = collections of (x1, x2,â€¦ ) ïƒ¨Prob of A = 1. Weak Law: é€™æ˜¯convergence in probabilityçš„æ¦‚ï¦£ Strong Law: For every Îµ>0 there exists n0, é€™æ˜¯convergence with probability 1 çš„æ¦‚ï¦£ 28 Pr(lim ) 1 n n M ï­ ï‚®ï‚¥ ï€½ ï€½ ï€¨ ï€© 1 2 / n n M X X X n ï€½ ï€« ï€« ï€« ïŒ 1 2 1 2 ( , , ) such that lim n n x x x x x n ï· ï­ ï‚®ï‚¥ ï€« ï€« ï€« ïƒ© ïƒ¹ ï€½ ï€½ ïƒª ïƒº ïƒ« ïƒ» ïŒ ïŒ ï» ï½ lim Pr( ) 0 n n M ï­ ï¥ ï‚®ï‚¥ ï€­ ï‚³ ï€½ ï» ï½ 0 Pr( : , ) 1 n n M n n ï· ï­ ï¥ ï€­"
  },
  {
    "source": "topic11.txt",
    "category": "Probability",
    "chunk_index": 27,
    "page": 27,
    "text": "ï­ ï¥ ï‚®ï‚¥ ï€­ ï‚³ ï€½ ï» ï½ 0 Pr( : , ) 1 n n M n n ï· ï­ ï¥ ï€­ ï€¼ ï‚³ ï€½"
  }
]