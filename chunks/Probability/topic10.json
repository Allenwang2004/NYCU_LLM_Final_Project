[
  {
    "source": "topic10.txt",
    "category": "Probability",
    "chunk_index": 0,
    "page": 1,
    "text": "Topic 10: Covariance and More on Conditional Expectation Outline ï¬ Covariance (å…±è®Šç•°æ•¸) and Correlation (ç›¸é—œæ€§) ï¬ Conditional Expectation ï¬ Total Expectation Theorem/ Total Variance ï¬ Bivariate Normal or Jointly Gaussian (é›™è®Šé‡å¸¸æ…‹åˆ†ä½ˆ) Reading: Chap. 4.2 ~ Chap. 4.3 1"
  },
  {
    "source": "topic10.txt",
    "category": "Probability",
    "chunk_index": 1,
    "page": 2,
    "text": "Covariance and Correlation Definition: The covariance of two random variables X and Y is denoted by cov(X, Y ), and is defined by When cov(X, Y ) = 0, we say that X and Y are uncorrelated. 2 Uncorrelated ä¸­æ–‡ç¿»æˆâ€œé›¶ç›¸é—œâ€ ç‚ºå¦¥(è€Œéâ€œä¸ç›¸é—œâ€) ï¼Œè¡¨ç¤ºcovariance=0 Question Uncorrelated RVs çš„ç‰©ç†æ„ç¾©ç‚ºä½•? å’Œindependent çš„å·®ç•°ä½•åœ¨?"
  },
  {
    "source": "topic10.txt",
    "category": "Probability",
    "chunk_index": 2,
    "page": 3,
    "text": "Interpretation of Covariance ï¬ Covariance provides a qualitative indicator of the relation between two random variables When cov(X,Y)>0, X-E[X] and Y-E[Y] â€œtendâ€ to have the same sign When cov(X,Y)<0, X-E[X] and Y-E[Y] â€œtendâ€ to have the opposite sign 3 (+,+) (â€“,â€“) å…©éš¨æ©Ÿè®Šæ•¸æ­£ç›¸é—œè¡¨ç¤º(å¹³ç§»è‡³æœŸæœ›å€¼å¾Œ)åŒè™Ÿçš„å‚¾å‘è¼ƒå¤§ï¼Œå¦‚ä¸Šåœ–(b) æ­£ç›¸é—œè¡¨ç¤º: è‹¥X-E[X] ç‚ºæ­£ï¼Œå‰‡Y-E[Y] ä¹Ÿç‚ºæ­£çš„æ©Ÿæœƒè¼ƒé«˜"
  },
  {
    "source": "topic10.txt",
    "category": "Probability",
    "chunk_index": 3,
    "page": 4,
    "text": "Uncorrelated and Independent é‡è¦: Independence implies uncorrelatedness. But the converse is generally NOT true. Example: (Uncorrelatedness does NOT imply independence) 4 ä¸Šè¿°å…©å¼ä¸¦éç­‰ç¾©!"
  },
  {
    "source": "topic10.txt",
    "category": "Probability",
    "chunk_index": 4,
    "page": 4,
    "text": "Correlation Coefficient The correlation coefficient Ï of two random variables X and Y that have nonzero variances is defined as ï¬A normalized version of the covariance ï¬We must have âˆ’1 â‰¤ğœŒğœŒâ‰¤1 (p.250: problem 20 and 21) ï¬If Ï >0, ğ‘‹ğ‘‹âˆ’ğ¸ğ¸ğ‘‹ğ‘‹and ğ‘Œğ‘Œâˆ’ğ¸ğ¸ğ‘Œğ‘Œâ€œtendâ€ to have the same sign , i.e. (+, +) or (âˆ’, âˆ’) ï¬The value of |Ï| provides a normalized measure of the extent to the relation between ğ‘‹ğ‘‹âˆ’ğ¸ğ¸ğ‘‹ğ‘‹and ğ‘Œğ‘Œâˆ’ğ¸ğ¸ğ‘Œğ‘Œ Example: Let ğ‘Œğ‘Œâˆ’ğ¸ğ¸ğ‘Œğ‘Œ= ğ‘ğ‘(ğ‘‹ğ‘‹âˆ’ğ¸ğ¸ğ‘‹ğ‘‹) for a positive constant c. Then, the correlation coefficient"
  },
  {
    "source": "topic10.txt",
    "category": "Probability",
    "chunk_index": 5,
    "page": 4,
    "text": "constant c. Then, the correlation coefficient ğœŒğœŒ= 1. This says that ğ‘‹ğ‘‹âˆ’ğ¸ğ¸ğ‘‹ğ‘‹exactly aligns with ğ‘Œğ‘Œâˆ’ğ¸ğ¸ğ‘Œğ‘Œ, subject to a positive constant (STRONG correlation) 5"
  },
  {
    "source": "topic10.txt",
    "category": "Probability",
    "chunk_index": 6,
    "page": 6,
    "text": "Variance of the Sum of Random Variables Several properties related to covariance: Variance of the Sum of Random Variables  For n=2: var(X1+X2)=var(X1) +2cov(X1,X2)+var(X2)  See textbook page 221 for an example. 6\n\nExample Consider the problem that n people throw their hat in a box and then pick a hat at random. Let the number of people who pick their own hat be denoted by X. Find var(X). First, define the indicator Xi as follows: We know P(Xi=1)=1/nï¼Œso Then we have 7"
  },
  {
    "source": "topic10.txt",
    "category": "Probability",
    "chunk_index": 7,
    "page": 8,
    "text": "Conditional Expectation and Total Expectation Recall: The conditional expectation E[X | Y = y] is defined by It should be noted that E[X | Y ] depends on Y, and is therefore a random variable whose value is E[X | Y = y] when the outcome of Y is y. Law of Iterated Expectations: (Total Expectation Theorem, p. 104, p. 173) 8 [ ] [ | ] ( ) [ | ] [ ] [ | ] ( ) Y y Y E X Y y p y E E X Y E X E X Y y f y dy âˆ âˆ’âˆ ï£± ï£¼ = ï£´ ï£´ = = ï£² ï£½ ï£´ ï£´ = ï£³ ï£¾ âˆ‘ âˆ«"
  },
  {
    "source": "topic10.txt",
    "category": "Probability",
    "chunk_index": 8,
    "page": 9,
    "text": "Example of E[ E[X | Y ] ]=E[X ] Orthogonality Principle: Show that for any function g(.). (Proof) We first need to show (See problem 25.) 9\n\nConditional Variance The conditional distribution of X given Y = y has a mean E[X | Y = y], and by the same token, it also has a variance defined as 1. Var(X|Y) is a function of Y, and is therefore a random variable 2. Law of total variance 10\n\nProof of Law of Total Variance 11"
  },
  {
    "source": "topic10.txt",
    "category": "Probability",
    "chunk_index": 9,
    "page": 12,
    "text": "Proof of Law of Total Variance 11\n\nIntuitions behind Law of Total Variance 12 Y X E[X] Y X When E ğ‘‹ğ‘‹ğ‘Œğ‘Œ= E ğ‘‹ğ‘‹, a constant When E ğ‘‹ğ‘‹ğ‘Œğ‘Œdepends on ğ‘Œğ‘Œ\n\nExample (1) 13\n\nExample (2) (From MIT Open CourseWare) 14"
  },
  {
    "source": "topic10.txt",
    "category": "Probability",
    "chunk_index": 10,
    "page": 12,
    "text": "Least Squares Estimation/Minimum Mean Square Error (MMSE) In many practical applications, we want to form an estimate of the value of a random variable X given the value of a related random variable Y, which may be viewed as some form of â€œmeasurementâ€ of X. (For example, estimate the distance of an object from a radar) A popular formulation is based on finding the estimate c, based on the observed Y, that minimizes the expected value of the squared error (X-c)2. This is called the least squares"
  },
  {
    "source": "topic10.txt",
    "category": "Probability",
    "chunk_index": 11,
    "page": 12,
    "text": "error (X-c)2. This is called the least squares estimation in our textbook, or minimum mean-square error (MMSE) estimation in a large body of literature Measurement Least Squares Estimator Observed (Measured, Received) Typically continuous Y=y X a function of Y 15"
  },
  {
    "source": "topic10.txt",
    "category": "Probability",
    "chunk_index": 12,
    "page": 16,
    "text": "MMSE Estimator The problem here is to find a function g(Y) such that the mean-square error (MSE) is minimized: It can be shown (Section 8.3) that out of all possible estimators g(Y), the MSE is minimized when That is This can be proved using the othogonality principle (page 9). 16"
  },
  {
    "source": "topic10.txt",
    "category": "Probability",
    "chunk_index": 13,
    "page": 17,
    "text": "Example 1. An IoT sensor is probing the temperature X on the top of Mt. Jade. It is known that the measurement of the IoT sensor is corrupted by Gaussian noise Z, where the measured signal Y is modeled by ğ‘Œğ‘Œ= ğ‘‹ğ‘‹+ ğ‘ğ‘, where ğ‘‹ğ‘‹~ğ‘ğ‘(0, ğœğœğ‘‹ğ‘‹ 2) and ğ‘ğ‘~ğ‘ğ‘(0, ğœğœğ‘ğ‘ 2) are independent normal RVs. What is the MMSE estimate of X provided the sensor has observed Y ? 17"
  },
  {
    "source": "topic10.txt",
    "category": "Probability",
    "chunk_index": 14,
    "page": 17,
    "text": "Bivariate (Jointly) Normal Distribution Jointly Normal Random Variables (p.254; problem 28*) Two random variables X and Y are said to be jointly normal if they can be expressed in the form where a, b,c and d are some scalars, U and V are independent normal random variables. Remarks: 1. (Recall) Any linear combination of U and V is Gaussian. So, X or Y is Gaussian 2. If X and Y are jointly Gaussian, then any linear combination Z=s1X+s2Y of X and Y has a Gaussian distribution (é€™æ˜¯å¦å¤–ä¸€ç¨®å®šç¾©bivariate"
  },
  {
    "source": "topic10.txt",
    "category": "Probability",
    "chunk_index": 15,
    "page": 17,
    "text": "Y has a Gaussian distribution (é€™æ˜¯å¦å¤–ä¸€ç¨®å®šç¾©bivariate normalçš„æ–¹å¼) 3. If X and Y are jointly normal, then either X or Y is marginally (individually) normal. However, the converse is in general not true. 18"
  },
  {
    "source": "topic10.txt",
    "category": "Probability",
    "chunk_index": 16,
    "page": 19,
    "text": "Uncorrelated Jointly Normal RVs Are Independent A VERY important property for jointly normal random variables: If two random variables X and Y are jointly normal and are uncorrelated, then they are independent. (See page 20 of this topic.) Note: Generally, two uncorrelated random variables are not necessarily independent. (See page 4 of this topic.) Example If X and Y are zero mean jointly normal with variances ÏƒX 2 and ÏƒY 2 respectively, find a constant Î± such that X-Î± Y and Y are independent."
  },
  {
    "source": "topic10.txt",
    "category": "Probability",
    "chunk_index": 17,
    "page": 19,
    "text": "constant Î± such that X-Î± Y and Y are independent. (Sol.) 19"
  },
  {
    "source": "topic10.txt",
    "category": "Probability",
    "chunk_index": 18,
    "page": 20,
    "text": "Conditional Distribution of Jointly Normal RVs Question Let X and Y have a bivariate (jointly) normal distribution (Assume zero mean, for simplicity). Then, what is the conditional density ğ‘“ğ‘“ğ‘‹ğ‘‹|ğ‘Œğ‘Œ(ğ‘¥ğ‘¥|ğ‘¦ğ‘¦) of X given Y Two approaches: 1) Use the conditional density formula; or 2) Decompose X into X=X-Î±Y+Î±Y, such that X-Î±Y and Y are indep. (X and Y are marginal normal) We adopt the 2nd approach here: 1. From last pageâ€™s example, the conditional expectation is given by It is a linear function of Y"
  },
  {
    "source": "topic10.txt",
    "category": "Probability",
    "chunk_index": 19,
    "page": 20,
    "text": "is given by It is a linear function of Y and therefore has a normal PDF 2. The error X-E[X|Y] is zero mean, normal, and independent of Y, with variance (1-Ï2)ÏƒX 2 3. The conditional distribution of X given Y is normal with mean E[X|Y] (given by the above eqn. (1) ) and variance (1-Ï2)ÏƒX 2 20 (1)"
  },
  {
    "source": "topic10.txt",
    "category": "Probability",
    "chunk_index": 20,
    "page": 21,
    "text": "Jointly Normal PDF Assume zero mean again for simplicity. From the previous page, the conditional distribution of X given Y is normal with mean E[X|Y] and variance (1-Ï2)ÏƒX 2 We can find the joint PDF fX,Y(x,y) of X and Y using the multiplication rule which takes the form where Note: When Ï=0, X and Y are uncorrelated. And the above PDF also reveals that X and Y are independent with Ï=0 (since joint PDF factors) 21"
  },
  {
    "source": "topic10.txt",
    "category": "Probability",
    "chunk_index": 21,
    "page": 22,
    "text": "Jointly Normal PDF Contours Ï=0.5 X Y Y X 22 Contour (ç­‰é«˜ç·šã€æ©«åˆ‡é¢åœ–) ğ‘“ğ‘“ğ‘‹ğ‘‹,ğ‘Œğ‘Œğ‘¥ğ‘¥, ğ‘¦ğ‘¦= constant ğ¶ğ¶1 ïƒ¨q(x,y) = constant ğ¶ğ¶2 What about a vertical slice section at Y=y? (ç¸±åˆ‡é¢åœ–)"
  },
  {
    "source": "topic10.txt",
    "category": "Probability",
    "chunk_index": 22,
    "page": 23,
    "text": "Example 1. An IoT sensor is probing the temperature X on the top of Mt. Jade. It is known that the measurement of the IoT sensor is corrupted by Gaussian noise Z, where the measured signal Y is modeled by ğ‘Œğ‘Œ= ğ‘‹ğ‘‹+ ğ‘ğ‘, where ğ‘‹ğ‘‹~ğ‘ğ‘(0, ğœğœğ‘‹ğ‘‹ 2) and ğ‘ğ‘~ğ‘ğ‘(0, ğœğœğ‘ğ‘ 2) are independent normal RVs. What is the MMSE estimate of X provided the sensor has observed Y ? 23"
  }
]