[
  {
    "source": "topic4.txt",
    "category": "Probability",
    "chunk_index": 0,
    "page": 1,
    "text": "Probability  Topic 4: Expectation and Variance Lecture Outline ï¬ Expectation (æœŸæœ›å€¼) ï¬Variance (è®Šç•°æ•¸) ï¬Expected Value Rule æˆ–ç¨±Law of the Unconscious Statistician (LOTUS) Reading: Textbook 2.4"
  },
  {
    "source": "topic4.txt",
    "category": "Probability",
    "chunk_index": 1,
    "page": 2,
    "text": "Probability  ï¬ A random variable X is a function defined on the sample space of an experiment ï¬ The PMF of a discrete RV X : You need to find pX (x) = P(X = x) for all x ï¬ Given a random variable X and a function g(.), we can define a new random variable Y = g(X), i.e., Y (Ï‰) = g(X (Ï‰)) is a mapping from Ï‰ to a real number The PMF of Y can be computed from the PMF of X : Review"
  },
  {
    "source": "topic4.txt",
    "category": "Probability",
    "chunk_index": 2,
    "page": 3,
    "text": "Probability  Expectation Definition The expected value E[X] (also called the average value, expectation, or mean æœŸæœ›å€¼ã€å¹³å‡å€¼) of a random variable X, with PMF pX(x), is defined by Example What is the expected value of rolling a fair dice? ç‰©ç†æ„ç¾©: The expected value can be regarded as the representative sample value (ä»£è¡¨å€¼) of an R.V. (e.g. the average score in a class). It needs not be exactly identical to a possible value of the R.V.\n\nProbability  Analogy to Center of Gravity"
  },
  {
    "source": "topic4.txt",
    "category": "Probability",
    "chunk_index": 3,
    "page": 5,
    "text": "Probability  Elementary Properties of Expectation ï¬If ğ‘‹ğ‘‹â‰¥0 , then E[ğ‘‹ğ‘‹] â‰¥0 ï¬If a â‰¤ğ‘‹ğ‘‹â‰¤ğ‘ğ‘, then a â‰¤ğ¸ğ¸[ğ‘‹ğ‘‹] â‰¤ğ‘ğ‘ ï¬If c is a constant, ğ¸ğ¸ğ‘ğ‘= ğ‘ğ‘\n\nProbability  Variance Definition The variance ï¼ˆè®Šç•°æ•¸ï¼‰of X is defined as the expected value of the random variable (X âˆ’ E[X])2 ï¬ ç‰©ç†æ„ç¾©: The variance provides a measure of dispersion (æ•£äº‚ç¨‹åº¦) of X around its mean. ï¬ Standard deviation (æ¨™æº–å·®) is defined as the square root of the variance ï¬ å¯¦å‹™ä¸Šï¼Œæ€éº¼è¨ˆç®—varianceå‘¢?"
  },
  {
    "source": "topic4.txt",
    "category": "Probability",
    "chunk_index": 4,
    "page": 7,
    "text": "Probability  å¦‚ä½•è¨ˆç®—è®Šç•°æ•¸Variance ï¬ åœŸæ³•ç…‰é‹¼æ³•ï¼šğ¸ğ¸[ ğ‘‹ğ‘‹âˆ’ğ¸ğ¸ğ‘‹ğ‘‹ 2] ä»¤ Y=(X-E[X])2ï¼Œæ‰¾å‡ºï¼¹çš„PMFï¼Œå‰‡å¯ç”¨æœŸæœ›å€¼çš„å®šç¾©è¨ˆç®—E[Y]ï¼Œå³æ˜¯var(X)ã€‚ ï¬ é€²éšä½œæ³•: ä½¿ç”¨Expected Value Rule Expected Value Rule (æˆ–ç¨±Law of the unconscious statistician, LOTUS) Let X be a random variable with PMF pX(x), and let g(X) be a real-valued function of X. Then ä»¤g(X) =(X-E[X])2 ä»£å…¥ä¸Šå¼æ±‚è§£ã€‚ ä¸Šè¿°LOTUS å¥½ç”¨ä¹‹è™•: å¯ç›´æ¥ä½¿ç”¨X çš„PMFä¾†è¨ˆç®—æœŸæœ›å€¼ï¼Œä¸éœ€è¦g(X) çš„PMF"
  },
  {
    "source": "topic4.txt",
    "category": "Probability",
    "chunk_index": 5,
    "page": 8,
    "text": "Probability  Expectation for Functions of Random Variables LOTUS Let X be a random variable with PMF pX(x), and let g(X) be a real-valued function of X. Then, ï¬ This result is extremely useful. It tells us, to find E[g(X)], we donâ€™t actually need the PMF of g(X). Instead, knowing the PMF of X can do the trick. Using the expected value rule and letting g(X)=(X âˆ’ E[X])2, we have the variance of X Similarly, the nth moment of X is Check the proof of LOTUS on page 85 of the textbook"
  },
  {
    "source": "topic4.txt",
    "category": "Probability",
    "chunk_index": 6,
    "page": 9,
    "text": "Probability  Example of LOTUS\n\nProbability  More on Variance ï¬ Variance of X can be expressed in terms of its first 2 moments1 é€™å€‹å¼å­ç‚ºè¨ˆç®—varianceæœ€å¸¸ç”¨çš„å…¬å¼ã€‚ ï¬ Variance of X is always nonnegative which yields the fact that 1Definition for Moments Sometimes we are interested in finding E[Xn ], which is called the n-th moment of the R.V. X. ä¹Ÿå°±æ˜¯ E[X2] æ˜¯2nd momentï¼ŒE[X7]æ˜¯7th moment"
  },
  {
    "source": "topic4.txt",
    "category": "Probability",
    "chunk_index": 7,
    "page": 11,
    "text": "Probability  Mean and Variance of a Linear Function of X Let X be a random variable and let where a and b are given (non-random) scalars. Then, Remark: Expectation is a linear operator, but variance is not.\n\nWhat is the constant ğ›¼ğ›¼that minimizes the mean square error (MSEï¼Œæœ€å°å‡æ–¹å·®) between a random variable X and ğ›¼ğ›¼. That is, please find the constant ğ›¼ğ›¼such that ğ¸ğ¸ ğ‘‹ğ‘‹âˆ’ğ›¼ğ›¼2 is minimized. Any intuition before you start? Probability  Example: the best fit to a random variable"
  },
  {
    "source": "topic4.txt",
    "category": "Probability",
    "chunk_index": 8,
    "page": 13,
    "text": "Probability  Mean and Variance of Bernoulli Bernoulli Random Variable\n\nProbability  Mean and Variance of Binomial Binomial Random Variables Binomial random variable X with parameter (n,p) has PMF Its mean and variance are This is left as an exercise for you. Hint: Use the identity\n\nProbability  Mean and Variance of Uniform Uniform Random Variables X takes on values in {1,2,â€¦,n} equally likely."
  },
  {
    "source": "topic4.txt",
    "category": "Probability",
    "chunk_index": 9,
    "page": 16,
    "text": "Probability  Mean and Variance of Uniform General Uniform Random Variables Suppose X takes on values in {a,a+1,â€¦,b} equally likely.\n\nProbability  Mean and Variance of Geometric Geometric Random Variables The PMF of Geometric X is given by Its mean is é€™æœŸæœ›å€¼çš„çµæœç¬¦åˆç›´è¦ºå—ï¼Ÿ And, its variance is\n\nProbability  Mean and Variance of Poisson Poisson Random Variable A Poisson random variable X with parameter Î» has PMF Its mean and variance are both equal to the parameter Î»."
  }
]