[
  {
    "source": "topic12.txt",
    "category": "Probability",
    "chunk_index": 0,
    "page": 1,
    "text": "Topic 12: Bayesian Statistical Inference Lecture Outline ï¬Introduction  Probability vs. Statistics  Bayesian vs. Classical Statistics ï¬Bayesian Statistical Inference Bayesian estimation Bayesian hypothesis testing (Bayesian detection) ï¬Maximum a Posteriori (MAP) Rule MAP estimation and MAP detection ï¬Least Mean Squares (LMS) and Linear LMS Estimation Reading : Textbook 8..4 1"
  },
  {
    "source": "topic12.txt",
    "category": "Probability",
    "chunk_index": 1,
    "page": 1,
    "text": "2 ï¬Probability â€“ an axiomatic mathematical theory ï¬Statistical inference â€“ estimate (or predict) something (unknown variables/model parameters/reality) based on the observed data Statistical inference â€“ many â€œmethodsâ€ have been proposed, depending on multitude of factors such as on the performance, e.g. minimum MSE or minimum error probability, or more generally minimum loss, the designer would like to achieve å·¥ç¨‹ç³»çµ±è¨­è¨ˆ(å¦‚æ‰‹æ©Ÿé€šè¨Šæ¼”ç®—æ³•)ï¼Œæˆ–æ˜¯é€éäººå·¥æ™ºæ…§è™•ç†çš„é ä¼°èˆ‡åˆ†é¡å•é¡Œå¹¾ä¹éƒ½æ˜¯åœ¨å¾—çŸ¥æŸ äº›äº‹ä»¶(å¦‚å·²çŸ¥é‡æ¸¬è¨Šè™Ÿã€è’é›†åˆ°çš„è³‡æ–™)çš„å‰æä¸‹ä½œå‡ºæ±ºç­–åˆ¤æ–· Probability"
  },
  {
    "source": "topic12.txt",
    "category": "Probability",
    "chunk_index": 2,
    "page": 1,
    "text": "äº›äº‹ä»¶(å¦‚å·²çŸ¥é‡æ¸¬è¨Šè™Ÿã€è’é›†åˆ°çš„è³‡æ–™)çš„å‰æä¸‹ä½œå‡ºæ±ºç­–åˆ¤æ–· Probability vs. Statistics"
  },
  {
    "source": "topic12.txt",
    "category": "Probability",
    "chunk_index": 3,
    "page": 3,
    "text": "3 Example: (Probability) çµ¦å®šç’°å¢ƒè³‡è¨Šï¼Œè¨ˆç®—ç‰¹å®šäº‹ä»¶æœƒç™¼ç”Ÿçš„æ©Ÿç‡ Ex1: Flipping a fair coin two times, the probability of two â€œheadsâ€ is 1/4 (Statistics) çµ¦å®šç‰¹å®šäº‹ä»¶(è§€å¯Ÿçµæœ)ï¼Œæ¨è«–å‡ºç’°å¢ƒè³‡è¨Šç‚ºä½•? Ex: æœ‰ä¸€éŠ…æ¿ä½†ä¸çŸ¥å…¶å‡ºç¾æ­£é¢æ©Ÿç‡, è¦å¦‚ä½•ä¼°è¨ˆå‡ºæ­¤å‡ºç¾æ­£é¢æ©Ÿç‡? ä½ çš„ç›´è¦ºåšæ³•ç‚ºä½•å‘¢? Probability vs. Statistics"
  },
  {
    "source": "topic12.txt",
    "category": "Probability",
    "chunk_index": 4,
    "page": 4,
    "text": "4 çµ±è¨ˆå­¸çš„å…©å¤§é–€æ´¾: Bayesian vs. Classical ï¬Bayesian vs. Classical Statistics Bayesian: Unknown parameter (model) is treated as a random variable. In this case, we need to assume a proper distribution, i.e. the prior distribution, for the unknown parameter Classical: Unknown parameter (model) is treated as a deterministic quantity Both Bayesian and classical methods may give identical results, particularly when the prior does not provide useful information Bayesian vs. Classical Statistics"
  },
  {
    "source": "topic12.txt",
    "category": "Probability",
    "chunk_index": 5,
    "page": 5,
    "text": "5 Problems of statistical interference can be divided into two types: estimation problem or detection (hypothesis testing) problem ï¬Estimation (or, regression in machine learning terminology) Estimation problem involves with deciding continuous-valued parameter(s) è‹¥æ¬²ä¼°è¨ˆçš„åƒæ•¸æ˜¯é€£çºŒå¯¦æ•¸ï¼Œæ­¤æ™‚è¢«ç¨±ä½œæ˜¯estimation æˆ–æ˜¯regression çš„å•é¡Œ Ex: We employ a polynomial model to predict tomorrowâ€™s stock value. Then, we need to find the coefficients of the specified polynomial ï¬Detection (or hypothesis testing) (or"
  },
  {
    "source": "topic12.txt",
    "category": "Probability",
    "chunk_index": 6,
    "page": 5,
    "text": "polynomial ï¬Detection (or hypothesis testing) (or classification, in machine learning terminology) Dection problem involves with deciding finite discrete-valued parameter(s) è‹¥æ¬²ç ´è§£çš„åƒæ•¸ç‚ºé›¢æ•£å¯æ•¸ï¼Œæ­¤æ™‚è¢«ç¨±ä½œæ˜¯detection æˆ–æ˜¯regression çš„å•é¡Œ Ex: A smart phone decides whether â€œ0â€ or â€œ1â€ is transmitted in digital communications Statistical Inference Problems"
  },
  {
    "source": "topic12.txt",
    "category": "Probability",
    "chunk_index": 7,
    "page": 6,
    "text": "6 å…©å¤§é–€æ´¾éƒ½å„è‡ªæœ‰å°ä»˜estimation å’Œdetection çš„æ‰‹æ®µ ï¬Bayesian Statistical Inference ïƒ Chapter 8 Bayesian Estimation 1) Maximum a Posteriori Estimation (MAP estimation) ïƒ Section 8.1, 8.2 2) Least Mean-Square Error Estimation (LSE or MMSE) ïƒ Section 8.3, 8.4 Bayesian Detection 1) Maximum a Posteriori detection (MAP detection) ïƒ Section 8.1, 8.2 ä¸Šè¿°é€™ä¸‰ç¨®Baysian Inference éƒ½ç‰½æ¶‰åˆ°è¨ˆç®—Posterior Probability/Density ï¬Classical Statistics ïƒ Chapter 9 Classical Estimation / Classical Hypothesis Testing Statistical Inference"
  },
  {
    "source": "topic12.txt",
    "category": "Probability",
    "chunk_index": 8,
    "page": 6,
    "text": "Hypothesis Testing Statistical Inference Problems ç ”ç©¶æ‰€èª²ç¨‹ã€æª¢æ¸¬èˆ‡ä¼°è¨ˆã€‘(detection and estimation) æœ‰æ›´ç‚ºæ·±å…¥çš„æ¢è¨!"
  },
  {
    "source": "topic12.txt",
    "category": "Probability",
    "chunk_index": 9,
    "page": 7,
    "text": "7 ï¬Observed data â€œXâ€ are noisy (corrupted by â€œNâ€) ï¬Î¸: unknown deterministic (continuous) parameter : an estimator of Î¸ that depends on X ï¬Example: Given observations: An estimate of Î¸ : Performance? Classical Statistics 2 : i.i.d. with zero mean and variance i i i X N N Î¸ Ïƒ = + 1 2 , , n X X X ïŒ Ë†Î˜"
  },
  {
    "source": "topic12.txt",
    "category": "Probability",
    "chunk_index": 10,
    "page": 7,
    "text": "8 ï¬The desired but unknown variable Î˜: In the Bayesian framework, Î˜ is modeled as a RV (may be discrete or continuous) We need to assume a prior distribution pÎ˜(Î¸) (a prioriï¼Œäº‹å‰æ©Ÿç‡for discrete Î˜ ã€äº‹å‰æ©Ÿç‡å¯†åº¦for continuous Î˜) ï¬Goal: Estimate Î˜ using the observed data X Bayesian approach needs posterior distribution pÎ˜|X(Î¸|x) to update our understanding about Î˜ (pÎ˜|X(Î¸|x) can be äº‹ å¾Œæ©Ÿç‡for discrete Î˜ ã€äº‹å¾Œæ©Ÿç‡å¯†åº¦for continuous Î˜) Finding the posterior distribution pÎ˜|X(Î¸|x) relies on ïƒ¼ Bayesâ€™ rule ïƒ¼ A system"
  },
  {
    "source": "topic12.txt",
    "category": "Probability",
    "chunk_index": 11,
    "page": 7,
    "text": "pÎ˜|X(Î¸|x) relies on ïƒ¼ Bayesâ€™ rule ïƒ¼ A system model A system model describes how observation X is mathematically related to Î˜, which specifically provides the likelihood function ğ¿ğ¿ğœƒğœƒâ‰¡ğ‘ğ‘ğ‘‹ğ‘‹|Î˜(ğ‘¥ğ‘¥|ğœƒğœƒ), when given a fixed observed value at ğ‘‹ğ‘‹= ğ‘¥ğ‘¥ Bayesian Statistics"
  },
  {
    "source": "topic12.txt",
    "category": "Probability",
    "chunk_index": 12,
    "page": 9,
    "text": "9 ï¬â€œAâ€ is late in a date. The late time is an RV X, uniformly distributed over the interval [0, Î¸]. ï¬The parameter Î¸ is unknown and is modeled as an RV Î˜, which is uniformly distributed over [0,1]. ï¬After one date, we observe an event of X=x, say, 0.32 hours. How do we use this information to update the distribution of Î˜? (Sol) We look for fÎ˜|X(Î¸|x). Example (8.2, p.414)"
  },
  {
    "source": "topic12.txt",
    "category": "Probability",
    "chunk_index": 13,
    "page": 10,
    "text": "10 Bayes rule is of critical importance in the MAP detection/estimation problem ï¬There are 4 versions of Bayesâ€™ rules (textbook p. 181 and p. 413), depending on whether unknown variable Î˜ is discrete or continuous observation (data) X is discrete or continuous ï¬Hypothesis Testing (unknown discrete parameter Î˜) Discrete data X **Continuous data X Bayesâ€™ Rules (1) | | ( ) ( | ) ( | ) ( ) X X X p p x p x p x Î¸ Î¸ Î¸ Î˜ Î˜ Î˜ = | | ( ) ( | ) ( | ) ( ) X X X p f x p x f x Î¸ Î¸ Î¸ Î˜ Î˜ Î˜ ="
  },
  {
    "source": "topic12.txt",
    "category": "Probability",
    "chunk_index": 14,
    "page": 11,
    "text": "11 ï¬Estimation (unknown continuous parameter Î˜) **Discrete data X Ex: A coin with unknown bias (probability of head) -- Observe X heads in n tosses Continuous data X Bayesâ€™ Rules (2) | | ( ) ( | ) ( | ) ( ) X X X f f x f x f x Î¸ Î¸ Î¸ Î˜ Î˜ Î˜ = | | ( ) ( | ) ( | ) ( ) X X X f p x f x p x Î¸ Î¸ Î¸ Î˜ Î˜ Î˜ ="
  },
  {
    "source": "topic12.txt",
    "category": "Probability",
    "chunk_index": 15,
    "page": 12,
    "text": "12 1) Start with a prior distribution, pÎ˜(Î¸), for the unknown random variable Î˜ 2) A model that describes the relation between the observation vector X and the unknown Î˜, which allows for calculation of the likelihood function pX|Î˜ (x|Î¸) 3) After observing the value x of X, evaluate the posterior distribution of pÎ˜|X(Î¸|x), using the appropriate version of Bayesâ€™ rule. (p.413) Bayesian Inference Procedure"
  },
  {
    "source": "topic12.txt",
    "category": "Probability",
    "chunk_index": 16,
    "page": 13,
    "text": "13 ï¬ Having obtained the posterior distribution of pÎ˜|X(Î¸|x), we find the value of Î¸ with the maximum posterior prob./pdf ï¬For discrete Î˜, this is called MAP detection (MAP hypothesis testing) ï¬For continuous Î˜, this is called MAP estimation Maximum a Posteriori Probability (MAP) Rule | Ë† arg max ( | ) X p x Î¸ Î¸ Î¸ Î˜ = | Ë† arg max ( | ) X f x Î¸ Î¸ Î¸ Î˜ = Éµ Ì‚ğœƒğœƒ Ì‚ğœƒğœƒ"
  },
  {
    "source": "topic12.txt",
    "category": "Probability",
    "chunk_index": 17,
    "page": 14,
    "text": "14 ï¬Conditional Expectation: E(Î˜|X=x), the MMSE estimator (in page 16, Topic 10), i.e., the least mean squares estimator (Sec 8.3) ï¬Ex: (8.7, p.424) â€œAâ€ is late in a date, â€¦ (i) MAP: ïƒ¨ . (ii) Conditional Mean: MAP Rule vs. Conditional Expectation | 1 ( | ) , if 1 log X f x x x Î¸ Î¸ Î¸ Î˜ = â‰¤ â‰¤ â‹… 1 1 log ( | ) 1 log x x d E X x x x Î¸ Î¸ Î¸ â‹… Î˜ = = âˆ’ = âˆ« Ë† x Î¸ ="
  },
  {
    "source": "topic12.txt",
    "category": "Probability",
    "chunk_index": 18,
    "page": 14,
    "text": "15 Problem formulation: RV Î˜ takes one of m values, Î¸1, â€¦, Î¸m. Once the measurement RV X is observed (with value x), weâ€™d like to decide which hypothesis (one out of Î¸1, â€¦, Î¸m) is true. ï¬ MAP detection rule: Pick up Î¸i based on the maximum pÎ˜|X(Î¸i |x). ï¬ The case of m =2 is called the MAP binary hypothesis test (null and alternate) H0: Î˜ = Î¸1 H1: Î˜ = Î¸2 ï¬Tie: If there is a tie, either can be selected arbitrarily. ï¬MAP detection is the decision rule that minimizes the probability of incorrect"
  },
  {
    "source": "topic12.txt",
    "category": "Probability",
    "chunk_index": 19,
    "page": 14,
    "text": "rule that minimizes the probability of incorrect decision More on MAP Hypothesis Testing"
  },
  {
    "source": "topic12.txt",
    "category": "Probability",
    "chunk_index": 20,
    "page": 16,
    "text": "16 ï¬MAP detection is the decision rule that minimizes the probability of error decision. (p. 420) Remarks:  This theorem lays the foundations for signal detection in modern digital communication systems (4G, 5G, 6G and beyond) and for objects classification in machine learning/artificial intelligence (AI) applications!  In most cases, we are interested not only in designing MAP criterion, but also in knowing the corresponding min. probability of error decision. See Example 3.8, Example 8.9, and"
  },
  {
    "source": "topic12.txt",
    "category": "Probability",
    "chunk_index": 21,
    "page": 16,
    "text": "error decision. See Example 3.8, Example 8.9, and Problem 4 of HW 5. More on MAP Hypothesis Testing"
  },
  {
    "source": "topic12.txt",
    "category": "Probability",
    "chunk_index": 22,
    "page": 17,
    "text": "17 ï¬Two coins: Coin 1 (Î˜ =1): p1 (head) = 0.46 Coin 2 (Î˜ =2): p2 (head) = 0.52 Let pÎ˜(Î¸=1) = pÎ˜(Î¸=2) = 0.5. And X is the number of observed heads in n tosses. That is, the outcome of one toss, X=1 (head) or 0 (tail). (a) Decide which coin is selected with one observation, say, â€œtailâ€ (X=0). (b) Decide which coin is selected with n tosses and k heads appear (X=k). Sol) Calculate pÎ˜|X(Î¸|x) = pÎ˜(Î¸)pX|Î˜ (x|Î¸) . Now, because pÎ˜(1) = pÎ˜(2) we only need to calculate and compare pX|Î˜ (x|Î¸) for Î¸ =1 and"
  },
  {
    "source": "topic12.txt",
    "category": "Probability",
    "chunk_index": 23,
    "page": 17,
    "text": "to calculate and compare pX|Î˜ (x|Î¸) for Î¸ =1 and Î¸ =2. (a) pX|Î˜ (x=tail|Î¸=1) = .46 = 0.54 pX|Î˜ (x=tail|Î¸=2) =.52 = 0.48 Example (8.9, p.426)"
  },
  {
    "source": "topic12.txt",
    "category": "Probability",
    "chunk_index": 24,
    "page": 18,
    "text": "18 Sol) (b) n tosses and k heads, geometric distribution Prob: pX|Î˜ (x=k|Î¸=1) =ğ‘ğ‘1 ğ‘˜ğ‘˜1 âˆ’ğ‘ğ‘1 ğ‘›ğ‘›âˆ’ğ‘˜ğ‘˜ pX|Î˜ (x=k|Î¸=2) =ğ‘ğ‘2 ğ‘˜ğ‘˜1 âˆ’ğ‘ğ‘2 ğ‘›ğ‘›âˆ’ğ‘˜ğ‘˜ Error analysis: threshold k*: * * ( ) ( 1, ) ( 2, ) P error P X k P X k = Î˜ = > + Î˜ = â‰¤"
  },
  {
    "source": "topic12.txt",
    "category": "Probability",
    "chunk_index": 25,
    "page": 19,
    "text": "19 ï¬Estimate a random value using a constant. Goal: Find c=g(X) that minimizes the mean squared error Ex: Estimate R.V. Î˜ using c. minimize ğ¸ğ¸ Î˜ âˆ’ğ‘ğ‘2 ïƒ¨Ì‚ğ‘ğ‘= ğ¸ğ¸Î˜ (pf) ğ¸ğ¸ Î˜ âˆ’ğ‘ğ‘2 = ğ¸ğ¸Î˜2 âˆ’2ğ‘ğ‘ğ‘ğ‘Î˜ + ğ‘ğ‘2 minimize âˆ’2ğ‘ğ‘ğ‘ğ‘Î˜ + ğ‘ğ‘2 ïƒ take derivative âˆ’ğ¸ğ¸Î˜ + ğ‘ğ‘= 0 Optimal MSE in this case: ğ¸ğ¸ Î˜ âˆ’ğ¸ğ¸[Î˜] 2 = var(Î˜) Least Mean Squares Estimation â€“ No Observation"
  },
  {
    "source": "topic12.txt",
    "category": "Probability",
    "chunk_index": 26,
    "page": 19,
    "text": "20 ï¬Two RVâ€™s Î˜ and X; estimate Î˜ based on an observation X =x. Goal: Find g(X) that minimizes the mean squared error minimize ğ¸ğ¸ Î˜ âˆ’g(X) 2|ğ‘‹ğ‘‹= ğ‘¥ğ‘¥ ïƒ¨We have proved in Topic 10 that à· ğœƒğœƒğ¿ğ¿ğ¿ğ¿ğ¿ğ¿= ğ¸ğ¸Î˜|ğ‘‹ğ‘‹= ğ‘¥ğ‘¥ This is true for any x value of X. Thus, the least mean squares (LMS) estimator of Î˜ is conditional mean: ğ¸ğ¸Î˜|ğ‘‹ğ‘‹ That is, out of all estimators g(X) of Î˜ based on X, ğ¸ğ¸Î˜|ğ‘‹ğ‘‹gives the smallest mean squared error. ğ¸ğ¸ Î˜ âˆ’ğ¸ğ¸[Î˜|ğ‘‹ğ‘‹] 2 â‰¤ğ¸ğ¸ Î˜ âˆ’ğ‘”ğ‘”(ğ‘‹ğ‘‹) 2 Finding ğ¸ğ¸Î˜|ğ‘‹ğ‘‹requires? Least Mean Squares Estimation"
  },
  {
    "source": "topic12.txt",
    "category": "Probability",
    "chunk_index": 27,
    "page": 19,
    "text": "ğ¸ğ¸Î˜|ğ‘‹ğ‘‹requires? Least Mean Squares Estimation â€“ Based on X"
  },
  {
    "source": "topic12.txt",
    "category": "Probability",
    "chunk_index": 28,
    "page": 21,
    "text": "21 ï¬Two RVâ€™s: Î˜ and X = Î˜ +W Î˜ : uniform over [4,10] W: uniform over [-1,1] indep of Î˜ What is ğ¸ğ¸Î˜|ğ‘‹ğ‘‹? Sol) Example (8.11, p.432) (1/2)"
  },
  {
    "source": "topic12.txt",
    "category": "Probability",
    "chunk_index": 29,
    "page": 22,
    "text": "22 Sol) (a) ğ‘“ğ‘“Î˜,ğ‘‹ğ‘‹ğœƒğœƒ, ğ‘¥ğ‘¥= ğ‘“ğ‘“Î˜ ğœƒğœƒğ‘“ğ‘“ğ‘‹ğ‘‹|Î˜ ğ‘¥ğ‘¥ğœƒğœƒ= 1 6 1 2 = 1 12 ğ‘“ğ‘“Î˜|ğ‘‹ğ‘‹ğœƒğœƒğ‘¥ğ‘¥= àµ— ğ‘“ğ‘“Î˜,ğ‘‹ğ‘‹ğœƒğœƒ,ğ‘¥ğ‘¥ ğ‘“ğ‘“ğ‘‹ğ‘‹ğ‘¥ğ‘¥ = àµ˜ 1 12 ğ‘“ğ‘“ğ‘‹ğ‘‹ğ‘¥ğ‘¥=uiform Thus, pick up an x, all nonzero-prob Î¸ values form a vertical section [x-1,x+1]. (X-W) ğ¸ğ¸Î˜|ğ‘‹ğ‘‹= ğ‘¥ğ‘¥is the midpoint of that section. (b) Mean squared error: ğ¸ğ¸ Î˜ âˆ’ğ¸ğ¸[Î˜|ğ‘‹ğ‘‹= ğ‘¥ğ‘¥] 2 ğ‘¥ğ‘¥âˆˆ5,9 , MSE = àµ— 22 12 = 1 3 ğ‘¥ğ‘¥âˆˆ3,5 , MSE = àµ— (ğ‘¥ğ‘¥+1âˆ’4)2 12 = àµ— (ğ‘¥ğ‘¥âˆ’3)2 12 Example (8.11, p.432) (2/2)"
  },
  {
    "source": "topic12.txt",
    "category": "Probability",
    "chunk_index": 30,
    "page": 23,
    "text": "Properties of the Estimation Error Let denote the least squares estimator and the associated estimation error à·¨ğ‘‹ğ‘‹, respectively. Both of the above are random variables. The following properties hold: MMSE estimator is unbiased: ğ¸ğ¸à· ğ‘‹ğ‘‹= ğ¸ğ¸[ğ‘‹ğ‘‹] (or, equiv., ğ¸ğ¸à·¨ğ‘‹ğ‘‹= 0 ) (unbiased çš„å®šç¾©: ä¼°è¨ˆçµæœèˆ‡åŸæœ¬æ‰€æ¬²ä¼°è¨ˆåƒæ•¸æœ‰ç›¸åŒæœŸæœ›å€¼) Estimator is uncorrelated with error: cov( à· ğ‘‹ğ‘‹, à·¨ğ‘‹ğ‘‹) = 0 Power conservation: àµ¯ var(ğ‘‹ğ‘‹) = var( à· ğ‘‹ğ‘‹) + var( à·¨ğ‘‹ğ‘‹ 23"
  },
  {
    "source": "topic12.txt",
    "category": "Probability",
    "chunk_index": 31,
    "page": 23,
    "text": "24 ï¬Two RVâ€™s Î˜ and X; estimate Î˜ based on an observation X. The function g(X) that minimizes the mean squared error minimize ğ¸ğ¸ Î˜ âˆ’g(X) 2 is given by à· ğœƒğœƒğ¿ğ¿ğ¿ğ¿ğ¿ğ¿= ğ¸ğ¸Î˜|ğ‘‹ğ‘‹ This conditional mean very often is nonlinear in X, or does not have closed-form expression ï¬It is desirable to find g(X) that is constrained to be linear in X ğ‘”ğ‘”ğ‘‹ğ‘‹= ğ‘ğ‘ğ‘ğ‘+ ğ‘ğ‘ We wish to find ğ‘ğ‘and ğ‘ğ‘such that ğ¸ğ¸ Î˜ âˆ’ğ‘ğ‘ğ‘ğ‘âˆ’ğ‘ğ‘2 is minimized. This is called linear LMS estimation or linear MMSE (LMMSE). Linear Least Mean Squares"
  },
  {
    "source": "topic12.txt",
    "category": "Probability",
    "chunk_index": 32,
    "page": 23,
    "text": "or linear MMSE (LMMSE). Linear Least Mean Squares Estimation"
  },
  {
    "source": "topic12.txt",
    "category": "Probability",
    "chunk_index": 33,
    "page": 25,
    "text": "25 ï¬We wish to find ğ‘ğ‘and ğ‘ğ‘such that ğ¸ğ¸ Î˜ âˆ’ğ‘ğ‘ğ‘ğ‘âˆ’ğ‘ğ‘2 is minimized. Fixed ğ‘ğ‘, we have the best ğ‘ğ‘given by ğ‘ğ‘= ğ¸ğ¸Î˜ âˆ’ğ‘ğ‘ğ‘ğ‘[ğ‘‹ğ‘‹] With this ğ‘ğ‘, it remains to minimize ğ¸ğ¸ Î˜ âˆ’ğ‘ğ‘ğ‘ğ‘âˆ’(ğ¸ğ¸Î˜ âˆ’ğ‘ğ‘ğ‘ğ‘ğ‘‹ğ‘‹) 2 , which is exactly var(Î˜ âˆ’ğ‘ğ‘ğ‘ğ‘) The best ğ‘ğ‘minimizing the above is ğ‘ğ‘= ğœŒğœŒğœğœÎ˜ ğœğœğ‘‹ğ‘‹ We therefore have Linear Least Mean Squares Estimation"
  },
  {
    "source": "topic12.txt",
    "category": "Probability",
    "chunk_index": 34,
    "page": 26,
    "text": "26 ï¬The corresponding MSE is ï¬We can re-arrange the LMS estimator as This allows an interesting interpretation: The normalized X and à· ğœƒğœƒğ¿ğ¿ğ¿ğ¿ğ¿ğ¿ğ¿ğ¿is proportional to each other, subject to a scaling factor ğœŒğœŒ This is reasonable as 1) We are searching a linear relation 2) The similarity between X and Î˜ is described by the correlation coefficient ğœŒğœŒ Linear Least Mean Squares Estimation"
  },
  {
    "source": "topic12.txt",
    "category": "Probability",
    "chunk_index": 35,
    "page": 27,
    "text": "27 ï¬â€œAâ€ is late in a date. The late time is an RV X, uniformly distributed over the interval [0, Î¸]. ï¬The parameter Î¸ is unknown and is modeled as an RV Î˜, which is uniformly distributed over [0,1]. ï¬What is the linear LMS estimator of Î˜ based on X? Example 8.15, p. 439"
  },
  {
    "source": "topic12.txt",
    "category": "Probability",
    "chunk_index": 36,
    "page": 27,
    "text": "28 ï¬In many applications, we have more than one observations ğ‘‹ğ‘‹1, ğ‘‹ğ‘‹2, â€¦ , ğ‘‹ğ‘‹ğ‘›ğ‘›in order to estimate one single parameter Î˜ We still can to find g(X) that is constrained to be linear in ğ‘‹ğ‘‹1, ğ‘‹ğ‘‹2, â€¦ , ğ‘‹ğ‘‹ğ‘›ğ‘› ğ‘”ğ‘”ğ‘‹ğ‘‹= ğ‘ğ‘1ğ‘‹ğ‘‹1 + ğ‘ğ‘2ğ‘‹ğ‘‹2 + â‹¯+ ğ‘ğ‘ğ‘›ğ‘›ğ‘‹ğ‘‹ğ‘›ğ‘›+ ğ‘ğ‘ We wish to find ğ‘ğ‘1, ğ‘ğ‘2, â€¦ , ğ‘ğ‘ğ‘›ğ‘›and ğ‘ğ‘such that ğ¸ğ¸ Î˜ âˆ’(ğ‘ğ‘1ğ‘‹ğ‘‹1 + ğ‘ğ‘2ğ‘‹ğ‘‹2 + â‹¯+ ğ‘ğ‘ğ‘›ğ‘›ğ‘‹ğ‘‹ğ‘›ğ‘›+ ğ‘ğ‘) 2 is minimized. These coefficients ğ‘ğ‘1, ğ‘ğ‘2, â€¦ , ğ‘ğ‘ğ‘›ğ‘›and ğ‘ğ‘can be determined by setting to zero its partial derivatives with respective to ğ‘ğ‘1, ğ‘ğ‘2, â€¦ , ğ‘ğ‘ğ‘›ğ‘›and ğ‘ğ‘"
  },
  {
    "source": "topic12.txt",
    "category": "Probability",
    "chunk_index": 37,
    "page": 27,
    "text": "with respective to ğ‘ğ‘1, ğ‘ğ‘2, â€¦ , ğ‘ğ‘ğ‘›ğ‘›and ğ‘ğ‘ Multiple Observations with Single Parameter"
  }
]