{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9becc70e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "tokenizer_1B = AutoTokenizer.from_pretrained(\"meta-llama/Llama-3.2-1B\")\n",
    "model_1B = AutoModelForCausalLM.from_pretrained(\"meta-llama/Llama-3.2-1B\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f14e07a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Give me a podcast content between two friends discussing their favorite books. Using format like A: I am a book lover B: I enjoy reading too! and then let the audience vote on which one of you is more insightful.\n",
      "The first time it's going to be an interview with The Book Club Society, where we'll discuss why this society exists in general (and what are some good ideas or suggestions for us to do) as well as how the specific chapter came about? It was really fun getting together to talk all things literature!\n",
      "Next up will likely be my conversation with another friend - he enjoys science fiction but has never read anything outside that genre... We hope everyone comes by our table at FIC 2017 and joins in!\n"
     ]
    }
   ],
   "source": [
    "input_text = \"Give me a podcast content between two friends discussing their favorite books. Using format like A: I am a book lover B: I enjoy reading too!\"\n",
    "inputs = tokenizer_1B(input_text, return_tensors=\"pt\")\n",
    "outputs = model_1B.generate(\n",
    "    **inputs,\n",
    "    max_new_tokens=800,    \n",
    "    min_length=100,         \n",
    "    do_sample=True,         \n",
    "    temperature=0.8,        \n",
    "    top_k=50,              \n",
    "    top_p=0.95,            \n",
    "    repetition_penalty=1.2, \n",
    "    no_repeat_ngram_size=3, \n",
    "    early_stopping=True,    \n",
    "    pad_token_id=tokenizer_1B.eos_token_id\n",
    ")\n",
    "generated_text = tokenizer_1B.decode(outputs[0], skip_special_tokens=True)\n",
    "print(generated_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3676afe3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fetching 2 files: 100%|██████████| 2/2 [11:36<00:00, 348.04s/it]\n",
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:39<00:00, 19.96s/it]\n"
     ]
    }
   ],
   "source": [
    "tokenizer_3B = AutoTokenizer.from_pretrained(\"meta-llama/Llama-3.2-3B\")\n",
    "model_3B = AutoModelForCausalLM.from_pretrained(\"meta-llama/Llama-3.2-3B\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d88ae649",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_text = \"Give me a podcast content between two friends discussing their favorite books. Using format like A: I am a book lover B: I enjoy reading too!\"\n",
    "inputs = tokenizer_3B(input_text, return_tensors=\"pt\")\n",
    "outputs = model_3B.generate(\n",
    "    **inputs,\n",
    "    max_new_tokens=800,    \n",
    "    min_length=100,         \n",
    "    do_sample=True,         \n",
    "    temperature=0.8,        \n",
    "    top_k=50,              \n",
    "    top_p=0.95,            \n",
    "    repetition_penalty=1.2, \n",
    "    no_repeat_ngram_size=3, \n",
    "    early_stopping=True,    \n",
    "    pad_token_id=tokenizer_3B.eos_token_id\n",
    ")\n",
    "generated_text = tokenizer_3B.decode(outputs[0], skip_special_tokens=True)\n",
    "print(generated_text)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "final_project",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
